\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Visualisation of one round of DECCS\citep {Miklautz2021}. (1) The encoder is used to embed data points $X$. (2) Clustering results are generated by applying ensemble members $\mathcal {E} = \{KM, \ldots , SC\}$ to $Z$. (3) Classifiers $g_i$ are trained to predict the corresponding cluster labels $\pi _i$ from $Z$. (4) $Z$ is updated via minimizing $\mathcal {L}$.}}{16}{figure.2.1}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces The framework of deep descriptive clustering (DDC). DDC consists of one clustering objective, one sub-symbolic explanation objective, and one self-generated objective to maximize the consistency between clustering and explanation modules.}}{17}{figure.2.2}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Architecture of the Deep Embedded Clustering (DEC) model. The model consists of a stacked autoencoder followed by a clustering layer. The autoencoder learns a low-dimensional representation of the data, which is then clustered using a clustering objective.}}{19}{figure.2.3}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Architecture of the Variational Autoencoder (VAE) model. The encoder maps input data to a latent space, and the decoder reconstructs the data from the latent space. The model is trained to minimize reconstruction loss and regularization loss.}}{19}{figure.2.4}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Evolution and relationships between deep clustering methods}}{26}{figure.2.5}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Our work at the intersection of three research areas}}{27}{figure.2.6}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Overview of our integrated methodology combining semantic supervision, representation learning, and consensus clustering}}{31}{figure.3.1}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Training loss for baseline Autoencoder (AE). The reconstruction loss decreases smoothly from approximately 0.032 to 0.006 over 30 epochs, indicating successful convergence of the reconstruction objective.}}{48}{figure.4.1}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Training loss for Constrained Autoencoder (CAE). Multiple loss components are shown: total loss (blue), reconstruction loss, and tag prediction loss. All components show decreasing trends over 30 epochs.}}{49}{figure.4.2}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Training loss for DECCS mode. \textbf {Notable pattern:} Periodic spikes occur every 5 epochs, corresponding to when the consensus matrix is rebuilt. The spikes indicate that the new consensus targets temporarily increase the loss before the model adapts.}}{50}{figure.4.3}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces PCA projection of baseline AE embeddings. Points are colored by cluster assignment. The embedding space shows some structure with outliers on the right side.}}{51}{figure.4.4}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces PCA projection of CAE embeddings. The distribution shows a similar pattern to AE with a concentrated core and extended outliers.}}{51}{figure.4.5}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces PCA projection of DECCS embeddings. The embedding space shows a roughly arc-shaped distribution, with colors representing cluster assignments that do not show clear separation.}}{52}{figure.4.6}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces t-SNE projection of DECCS embeddings colored by cluster assignment. Local structure is visible with groups of same-colored points forming coherent regions, particularly in peripheral areas.}}{53}{figure.4.7}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {B.1}{\ignorespaces PCA projection of baseline Autoencoder (AE) embeddings. Colors represent cluster assignments from K-Means. The distribution shows a concentrated core with outlier points extending to the right.}}{72}{figure.B.1}%
\contentsline {figure}{\numberline {B.2}{\ignorespaces PCA projection of Constrained Autoencoder (CAE) embeddings. Similar structure to AE with a dense core and extended outliers, suggesting tag supervision did not fundamentally change the embedding geometry.}}{73}{figure.B.2}%
\contentsline {figure}{\numberline {B.3}{\ignorespaces PCA projection of DECCS embeddings. The distribution forms a roughly arc-shaped pattern. Color gradients do not correspond to spatially distinct clusters.}}{74}{figure.B.3}%
\contentsline {figure}{\numberline {B.4}{\ignorespaces t-SNE projection of DECCS embeddings. Multiple distinct clusters are visible (see colorbar), with local structure showing groups of same-colored points forming coherent regions.}}{75}{figure.B.4}%
\contentsline {figure}{\numberline {B.5}{\ignorespaces Training loss for baseline Autoencoder over 30 epochs. Reconstruction loss (MSE) decreases smoothly from approximately 0.032 to 0.006.}}{76}{figure.B.5}%
\contentsline {figure}{\numberline {B.6}{\ignorespaces Training loss for Constrained Autoencoder. Multiple curves show total loss, reconstruction loss, and tag prediction loss components.}}{77}{figure.B.6}%
\contentsline {figure}{\numberline {B.7}{\ignorespaces Training loss for DECCS mode showing characteristic periodic spikes every 5 epochs corresponding to consensus matrix rebuilding.}}{78}{figure.B.7}%
\contentsline {figure}{\numberline {B.8}{\ignorespaces DECCS framework from \cite {Miklautz2021}. (1) Encoder embeds input data. (2) Ensemble clustering algorithms generate base partitions. (3) Classifiers approximate partitions. (4) Representation is updated via consensus loss.}}{80}{figure.B.8}%
\contentsline {figure}{\numberline {B.9}{\ignorespaces Deep Descriptive Clustering (DDC) framework from \cite {Zhang2021}. Combines clustering objective with class-level explanations via Integer Linear Programming.}}{81}{figure.B.9}%
