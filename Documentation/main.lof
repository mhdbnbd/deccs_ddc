\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Visualisation of one round of DECCS\citep {Miklautz2021}. (1) The encoder is used to embed data points $X$. (2) Clustering results are generated by applying ensemble members $\mathcal {E} = \{KM, \ldots , SC\}$ to $Z$. (3) Classifiers $g_i$ are trained to predict the corresponding cluster labels $\pi _i$ from $Z$. (4) $Z$ is updated via minimizing $\mathcal {L}$.}}{17}{figure.2.1}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces The framework of deep descriptive clustering (DDC). DDC consists of one clustering objective, one sub-symbolic explanation objective, and one self-generated objective to maximize the consistency between clustering and explanation modules.}}{18}{figure.2.2}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Architecture of the Deep Embedded Clustering (DEC) model. The model consists of a stacked autoencoder followed by a clustering layer. The autoencoder learns a low-dimensional representation of the data, which is then clustered using a clustering objective.}}{20}{figure.2.3}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Architecture of the Variational Autoencoder (VAE) model. The encoder maps input data to a latent space, and the decoder reconstructs the data from the latent space. The model is trained to minimize reconstruction loss and regularization loss.}}{21}{figure.2.4}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Evolution and relationships between deep clustering methods}}{28}{figure.2.5}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Our work at the intersection of three research areas}}{30}{figure.2.6}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Overview of our integrated methodology combining semantic supervision, representation learning, and consensus clustering}}{33}{figure.3.1}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Training loss for baseline Autoencoder (AE). The reconstruction loss decreases smoothly from approximately 0.032 to 0.006 over 30 epochs, indicating successful convergence of the reconstruction objective.}}{49}{figure.4.1}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Training loss for Constrained Autoencoder (CAE). Multiple loss components are shown: total loss (blue), reconstruction loss, and tag prediction loss. All components show decreasing trends over 30 epochs.}}{50}{figure.4.2}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Training loss for DECCS mode. \textbf {Notable pattern:} Periodic spikes occur every 5 epochs, corresponding to when the consensus matrix is rebuilt. The spikes indicate that the new consensus targets temporarily increase the loss before the model adapts.}}{51}{figure.4.3}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces PCA projection of baseline AE embeddings. Points are colored by cluster assignment. The embedding space shows some structure with outliers on the right side.}}{52}{figure.4.4}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces PCA projection of CAE embeddings. The distribution shows a similar pattern to AE with a concentrated core and extended outliers.}}{52}{figure.4.5}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces PCA projection of DECCS embeddings. The embedding space shows a roughly arc-shaped distribution, with colors representing cluster assignments that do not show clear separation.}}{53}{figure.4.6}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces t-SNE projection of DECCS embeddings colored by cluster assignment. Local structure is visible with groups of same-colored points forming coherent regions, particularly in peripheral areas.}}{54}{figure.4.7}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {B.1}{\ignorespaces Effect of consensus weight on clustering performance (with $\lambda _{tag}=1.0$)}}{71}{figure.B.1}%
\contentsline {figure}{\numberline {B.2}{\ignorespaces Effect of tag supervision weight on clustering performance (with $\lambda _{consensus}=0.2$)}}{71}{figure.B.2}%
\contentsline {figure}{\numberline {B.3}{\ignorespaces Training convergence with optimal hyperparameters}}{72}{figure.B.3}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {D.1}{\ignorespaces PCA projection of baseline autoencoder (AE) embeddings. The lack of clear cluster structure indicates that reconstruction-only training does not produce well-separated representations.}}{76}{figure.D.1}%
\contentsline {figure}{\numberline {D.2}{\ignorespaces PCA projection of Constrained Autoencoder (CAE) embeddings. Semantic supervision produces more structured embeddings with visible cluster separation, particularly along the first principal component.}}{77}{figure.D.2}%
\contentsline {figure}{\numberline {D.3}{\ignorespaces PCA projection of DECCS embeddings with consensus clustering. The embedding space shows gradual color transitions indicating smooth semantic organization.}}{78}{figure.D.3}%
\contentsline {figure}{\numberline {D.4}{\ignorespaces t-SNE projection of DECCS embeddings colored by cluster assignment. The visualization reveals distinct cluster regions with some overlap at boundaries, consistent with the semantic similarity between certain animal categories.}}{79}{figure.D.4}%
\contentsline {figure}{\numberline {D.5}{\ignorespaces Training loss for baseline autoencoder (AE). Smooth convergence with reconstruction loss only.}}{80}{figure.D.5}%
\contentsline {figure}{\numberline {D.6}{\ignorespaces Training loss for Constrained Autoencoder (CAE). Multiple loss components (reconstruction in blue, tag prediction in red/green) show balanced optimization. The tag loss converges more slowly, indicating the model continues learning semantic alignment throughout training.}}{81}{figure.D.6}%
\contentsline {figure}{\numberline {D.7}{\ignorespaces Training loss for DECCS mode. Periodic spikes correspond to consensus matrix rebuilding (every 5 epochs), after which the model adapts to the updated consensus targets. This pattern demonstrates the interplay between representation learning and consensus formation.}}{82}{figure.D.7}%
\contentsline {figure}{\numberline {D.8}{\ignorespaces Mean attribute values by category across all classes}}{87}{figure.D.8}%
