\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Visualisation of one round of DECCS\citep {Miklautz2021}. (1) The encoder is used to embed data points $X$. (2) Clustering results are generated by applying ensemble members $\mathcal {E} = \{KM, \ldots , SC\}$ to $Z$. (3) Classifiers $g_i$ are trained to predict the corresponding cluster labels $\pi _i$ from $Z$. (4) $Z$ is updated via minimizing $\mathcal {L}$.}}{15}{figure.2.1}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces The framework of deep descriptive clustering (DDC). DDC consists of one clustering objective, one sub-symbolic explanation objective, and one self-generated objective to maximize the consistency between clustering and explanation modules.}}{16}{figure.2.2}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Architecture of the Deep Embedded Clustering (DEC) model. The model consists of a stacked autoencoder followed by a clustering layer. The autoencoder learns a low-dimensional representation of the data, which is then clustered using a clustering objective.}}{18}{figure.2.3}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Architecture of the Variational Autoencoder (VAE) model. The encoder maps input data to a latent space, and the decoder reconstructs the data from the latent space. The model is trained to minimize reconstruction loss and regularization loss.}}{19}{figure.2.4}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Evolution and relationships between deep clustering methods}}{26}{figure.2.5}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Our work at the intersection of three research areas}}{27}{figure.2.6}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Overview of our integrated methodology combining semantic supervision, representation learning, and consensus clustering}}{31}{figure.3.1}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Training loss components for the Constrained Autoencoder (CAE) over 30 epochs. Blue: total loss ($\mathcal {L}_{total}$), orange: reconstruction loss ($\mathcal {L}_{recon}$), green: tag prediction loss ($\mathcal {L}_{tag}$) with weight $\lambda _{tag} = 0.5$. All components show decreasing trends, converging by approximately epoch 20.}}{47}{figure.4.1}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Training loss for Constrained Autoencoder (CAE). Multiple loss components are shown: total loss (blue), reconstruction loss, and tag prediction loss. All components show decreasing trends over 30 epochs. [to be updated]}}{48}{figure.4.2}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Training loss for DECCS mode. \textbf {Notable pattern:} Periodic spikes occur every 5 epochs, corresponding to when the consensus matrix is rebuilt. The spikes indicate that the new consensus targets temporarily increase the loss before the model adapts.}}{49}{figure.4.3}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces PCA projection of baseline autoencoder embeddings. The distribution forms a dense elliptical core with a prominent tail of outliers extending along PC1 (teal points, reaching PC1$\approx $40). These outliers likely represent visually distinctive samples that the reconstruction objective maps to extreme embedding values. Cluster colors are heavily mixed within the core, indicating the reconstruction-only objective does not naturally separate semantic categories.}}{50}{figure.4.4}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces PCA projection of Constrained Autoencoder (CAE) embeddings. Tag supervision reshapes the embedding geometry into an L-shaped distribution: a vertical arm extending upward (blue points, PC2$\approx $20) and a horizontal arm extending rightward (teal points, PC1$\approx $40). This bifurcation suggests the tag prediction branch creates distinct embedding regions for samples with different dominant attributes. However, cluster colors remain mixed in the dense core, indicating semantic supervision alone does not fully resolve cluster boundaries.}}{51}{figure.4.5}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces PCA projection of DECCS embeddings. The consensus mechanism produces a curved, arc-shaped distribution spanning PC1$\in $[-12, 18] and PC2$\in $[-25, 7]. Unlike AE and CAE, some spatial organization by cluster is visible: green/yellow points concentrate on the right, blue points in the upper region, and purple outliers extend downward. The arc geometry suggests the consensus loss constrains embeddings to a lower-dimensional manifold while encouraging cluster separation.}}{52}{figure.4.6}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces t-SNE projection of DECCS embeddings colored by cluster assignment (color bar indicates cluster IDs 0--30). The visualization reveals clear local structure: peripheral regions contain well-separated, single-color clusters (e.g., bright green at far left, orange at lower left, cyan at upper right), while the central region shows more mixing. This pattern is consistent with NMI=0.642---the clustering captures meaningful structure but does not perfectly align with all ground truth boundaries.}}{53}{figure.4.7}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.1}{\ignorespaces Effect of consensus weight on clustering performance (with $\lambda _{tag}=1.0$)}}{69}{figure.A.1}%
\contentsline {figure}{\numberline {A.2}{\ignorespaces Effect of tag supervision weight on clustering performance (with $\lambda _{consensus}=0.2$)}}{69}{figure.A.2}%
\contentsline {figure}{\numberline {A.3}{\ignorespaces Training convergence with optimal hyperparameters}}{70}{figure.A.3}%
\addvspace {10\p@ }
