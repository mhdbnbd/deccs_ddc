\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Abstract}{1}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Contents}{1}{chapter*.1}\protected@file@percent }
\citation{Miklautz2021}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{5}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Tables}{7}{chapter*.3}\protected@file@percent }
\citation{LeCun2015}
\citation{Ren2024}
\citation{Guidotti2018,Balachandran2009}
\citation{Saisubramanian2019}
\citation{Ozyegen2022}
\citation{Plant2011}
\citation{Tjoa2023}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{8}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Background}{8}{section.1.1}\protected@file@percent }
\citation{Ruder2017,Crawshaw2020}
\citation{Bengio2013}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Problem Statement}{9}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Formal Problem Definition}{9}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Key Challenges}{9}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Research Objectives}{10}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Contributions}{10}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Methodological Contributions}{10}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Empirical Contributions}{11}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Practical Contributions}{11}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Thesis Structure}{11}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Scope and Delimitations}{12}{section.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}In Scope}{12}{subsection.1.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Out of Scope}{12}{subsection.1.6.2}\protected@file@percent }
\citation{Miklautz2021}
\citation{Min2018,Zhou2024}
\citation{Vega-Pons2011}
\citation{Strehl2002,Fred2005}
\citation{Miklautz2021}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{13}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Deep Embedded Clustering with Consensus Representations (DECCS)}{13}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Related Work in Consensus Clustering and Deep Clustering}{13}{subsection.2.1.1}\protected@file@percent }
\citation{Xie2016}
\citation{Guo2017}
\citation{Jiang2017}
\citation{Chang2017}
\citation{Yang2016}
\citation{Caron2018}
\citation{Mautz2020}
\citation{Kingma2014}
\citation{Miklautz2021}
\citation{Miklautz2021}
\citation{Vinh2010}
\citation{Zhang2021}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Visualisation of one round of DECCS\citep  {Miklautz2021}. (1) The encoder is used to embed data points $X$. (2) Clustering results are generated by applying ensemble members $\mathcal  {E} = \{KM, \ldots  , SC\}$ to $Z$. (3) Classifiers $g_i$ are trained to predict the corresponding cluster labels $\pi _i$ from $Z$. (4) $Z$ is updated via minimizing $\mathcal  {L}$.}}{15}{figure.2.1}\protected@file@percent }
\newlabel{fig:pipeline}{{2.1}{15}{Visualisation of one round of DECCS\citep {Miklautz2021}. (1) The encoder is used to embed data points $X$. (2) Clustering results are generated by applying ensemble members $\mathcal {E} = \{KM, \ldots , SC\}$ to $Z$. (3) Classifiers $g_i$ are trained to predict the corresponding cluster labels $\pi _i$ from $Z$. (4) $Z$ is updated via minimizing $\mathcal {L}$}{figure.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Methodology of DECCS}{15}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Experimental Evaluation}{15}{subsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The framework of deep descriptive clustering (DDC). DDC consists of one clustering objective, one sub-symbolic explanation objective, and one self-generated objective to maximize the consistency between clustering and explanation modules.}}{16}{figure.2.2}\protected@file@percent }
\newlabel{fig:ddc}{{2.2}{16}{The framework of deep descriptive clustering (DDC). DDC consists of one clustering objective, one sub-symbolic explanation objective, and one self-generated objective to maximize the consistency between clustering and explanation modules}{figure.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Deep Descriptive Clustering (DDC)}{16}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Key Aspects of Deep Descriptive Clustering (DDC)}{16}{subsection.2.2.1}\protected@file@percent }
\citation{Xie2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Experiments and Findings}{17}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Conclusion and Future Directions}{17}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Related Works}{17}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Deep Embedded Clustering: A General Approach to Clustering Arbitrary Similarity Measures by J. Xie, R. Girshick, and A. Farhadi (2016)}{17}{subsection.2.3.1}\protected@file@percent }
\citation{Kingma2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Architecture of the Deep Embedded Clustering (DEC) model. The model consists of a stacked autoencoder followed by a clustering layer. The autoencoder learns a low-dimensional representation of the data, which is then clustered using a clustering objective.}}{18}{figure.2.3}\protected@file@percent }
\newlabel{fig:dec}{{2.3}{18}{Architecture of the Deep Embedded Clustering (DEC) model. The model consists of a stacked autoencoder followed by a clustering layer. The autoencoder learns a low-dimensional representation of the data, which is then clustered using a clustering objective}{figure.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Auto-Encoding Variational Bayes by D. P. Kingma and M. Welling (2014)}{18}{subsection.2.3.2}\protected@file@percent }
\citation{davidson2018}
\citation{Sambaturu2020}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Architecture of the Variational Autoencoder (VAE) model. The encoder maps input data to a latent space, and the decoder reconstructs the data from the latent space. The model is trained to minimize reconstruction loss and regularization loss.}}{19}{figure.2.4}\protected@file@percent }
\newlabel{fig:vae}{{2.4}{19}{Architecture of the Variational Autoencoder (VAE) model. The encoder maps input data to a latent space, and the decoder reconstructs the data from the latent space. The model is trained to minimize reconstruction loss and regularization loss}{figure.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Explainable-by-Design Algorithms}{19}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Post-processing Explanation Methods}{19}{subsection.2.3.4}\protected@file@percent }
\citation{Ribeiro2016}
\citation{Rishinanda2021,Liu2022,Chhajer2022}
\citation{Chen2020}
\citation{He2020}
\citation{Caron2020}
\citation{Grill2020}
\citation{Li2021}
\citation{Zhong2021}
\citation{Shen2021}
\citation{Jaiswal2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Other Related Works}{20}{subsection.2.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Contrastive Learning and Deep Clustering}{20}{subsection.2.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Comprehensive Comparison of Clustering Approaches}{21}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Taxonomy of Deep Clustering Methods}{21}{subsection.2.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Taxonomy of deep clustering approaches}}{21}{table.2.1}\protected@file@percent }
\newlabel{tab:taxonomy}{{2.1}{21}{Taxonomy of deep clustering approaches}{table.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Detailed Method Comparison}{21}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Deep Embedded Clustering (DEC)}{21}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Variational Autoencoders for Clustering (VAE-based)}{22}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Deep Clustering with Consensus Representations (DECCS)}{22}{subsection.2.4.2}\protected@file@percent }
\citation{Xie2016}
\citation{Miklautz2021}
\citation{Zhang2021}
\@writefile{toc}{\contentsline {subsubsection}{Deep Descriptive Clustering (DDC)}{23}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Quantitative Performance Comparison}{23}{subsection.2.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Performance comparison of deep clustering methods on benchmark datasets}}{23}{table.2.2}\protected@file@percent }
\newlabel{tab:methods_comparison}{{2.2}{23}{Performance comparison of deep clustering methods on benchmark datasets}{table.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Qualitative Feature Comparison}{24}{subsection.2.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Qualitative comparison of clustering method features}}{24}{table.2.3}\protected@file@percent }
\newlabel{tab:qualitative_comparison}{{2.3}{24}{Qualitative comparison of clustering method features}{table.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Computational Complexity Analysis}{24}{subsection.2.4.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Computational complexity comparison}}{24}{table.2.4}\protected@file@percent }
\newlabel{tab:complexity}{{2.4}{24}{Computational complexity comparison}{table.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.6}Applicability Analysis}{25}{subsection.2.4.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces Method applicability to different data types and scenarios}}{25}{table.2.5}\protected@file@percent }
\newlabel{tab:applicability}{{2.5}{25}{Method applicability to different data types and scenarios}{table.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.7}Key Innovations by Method}{25}{subsection.2.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.8}Limitations Comparison}{26}{subsection.2.4.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.6}{\ignorespaces Method limitations and failure modes}}{26}{table.2.6}\protected@file@percent }
\newlabel{tab:limitations}{{2.6}{26}{Method limitations and failure modes}{table.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Evolution of Deep Clustering Paradigms}{26}{section.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Evolution and relationships between deep clustering methods}}{26}{figure.2.5}\protected@file@percent }
\newlabel{fig:evolution}{{2.5}{26}{Evolution and relationships between deep clustering methods}{figure.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Research Gaps Addressed}{26}{subsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Positioning in the Literature}{27}{subsection.2.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Our work at the intersection of three research areas}}{27}{figure.2.6}\protected@file@percent }
\newlabel{fig:venn}{{2.6}{27}{Our work at the intersection of three research areas}{figure.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Summary and Implications for This Research}{28}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Key Insights from the Literature}{28}{subsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Research Gap}{28}{subsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Positioning of Our Work}{28}{subsection.2.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{30}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Overview}{30}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Methodological Contributions}{30}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Research Design}{30}{subsection.3.1.2}\protected@file@percent }
\citation{Xian2019,Lampert2014}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Overview of our integrated methodology combining semantic supervision, representation learning, and consensus clustering}}{31}{figure.3.1}\protected@file@percent }
\newlabel{fig:methodology_overview}{{3.1}{31}{Overview of our integrated methodology combining semantic supervision, representation learning, and consensus clustering}{figure.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Dataset Preparation}{31}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Animals with Attributes 2 (AwA2) Dataset}{31}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Semantic Attribute Structure}{31}{subsection.3.2.2}\protected@file@percent }
\citation{Bengio2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Data Preprocessing Pipeline}{33}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Image Preprocessing}{33}{subsection.3.2.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Image Preprocessing Pipeline}}{33}{algorithm.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Attribute Normalization}{33}{ALC@unique.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dataset Splitting}{33}{equation.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Data Loading Implementation}{33}{subsection.3.2.4}\protected@file@percent }
\citation{Akata2015,Akata2016}
\citation{Chen2021}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Model Architecture}{34}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Baseline Autoencoder (AE)}{34}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Encoder Architecture}{34}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Decoder Architecture}{34}{equation.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Baseline Loss Function}{34}{equation.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Constrained Autoencoder (CAE)}{34}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architecture Extension}{34}{subsection.3.3.2}\protected@file@percent }
\citation{Yang2017}
\@writefile{toc}{\contentsline {subsubsection}{Multi-Objective Loss Function}{35}{equation.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Hyperparameter Selection}{35}{equation.3.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Consensus Clustering Integration (DECCS)}{35}{subsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Consensus Representation Learning}{35}{subsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Ensemble Clustering Algorithms}{36}{equation.3.11}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Ensemble clustering algorithms and their characteristics}}{36}{table.3.1}\protected@file@percent }
\newlabel{tab:ensemble_algorithms}{{3.1}{36}{Ensemble clustering algorithms and their characteristics}{table.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Consensus Matrix Construction}{36}{table.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Consensus Consistency Loss}{36}{equation.3.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Full DECCS Loss}{36}{equation.3.15}\protected@file@percent }
\citation{Kingma2015}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Training Procedure}{37}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Optimization Strategy}{37}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Optimizer Configuration}{37}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Batch Processing}{37}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Mixed Precision Training}{37}{subsection.3.4.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Mixed Precision Training Step}}{37}{algorithm.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Training Algorithms}{38}{subsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Baseline Autoencoder Training}{38}{subsection.3.4.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Train Baseline Autoencoder}}{38}{algorithm.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Constrained Autoencoder Training}{38}{ALC@unique.20}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Train Constrained Autoencoder}}{38}{algorithm.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{DECCS Training with Consensus}{39}{ALC@unique.32}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Train DECCS with Consensus}}{39}{algorithm.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Clustering and Evaluation}{39}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Embedding Extraction}{39}{subsection.3.5.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Extract Embeddings}}{39}{algorithm.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Consensus Clustering}{39}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Ensemble Execution}{39}{subsection.3.5.2}\protected@file@percent }
\citation{Vinh2010}
\citation{Hubert1985}
\citation{Kuhn1955}
\citation{Rousseeuw1987}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Consensus Clustering Evaluation}}{40}{algorithm.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Evaluation Metrics}{40}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Clustering Performance Metrics}{40}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Normalized Mutual Information (NMI)}{40}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Adjusted Rand Index (ARI)}{40}{equation.3.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Clustering Accuracy (ACC)}{40}{equation.3.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Silhouette Score}{40}{equation.3.19}\protected@file@percent }
\citation{Paszke2019}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Cluster Explanation Generation}{41}{section.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Attribute-Based Characterization}{41}{subsection.3.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Top-K Attribute Selection}{41}{subsection.3.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Explanation Quality Metrics}{41}{subsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Cluster Purity}{41}{subsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Attribute Discriminativeness}{41}{equation.3.23}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Implementation Details}{41}{section.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Software Framework}{41}{subsection.3.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Hardware Configuration}{41}{subsection.3.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Reproducibility}{42}{subsection.3.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Limitations and Design Choices}{42}{section.3.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Methodological Limitations}{42}{subsection.3.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}Design Rationale}{42}{subsection.3.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Summary of Methodological Choices}{43}{section.3.9}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Complete configuration summary}}{43}{table.3.2}\protected@file@percent }
\newlabel{tab:table}{{3.2}{43}{Complete configuration summary}{table.3.2}{}}
\citation{Xian2019}
\citation{Kingma2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{44}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experimental Setup}{44}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Hyperparameters}{44}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Hyperparameter Tuning on Sample Subset}{44}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Grid Search Configuration}{45}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Sample-Based Tuning Results}{45}{subsection.4.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Hyperparameter tuning results on sample subset (N=160). \textbf  {Caution:} These results may not generalize to the full dataset.}}{45}{table.4.1}\protected@file@percent }
\newlabel{tab:tuning_results}{{4.1}{45}{Hyperparameter tuning results on sample subset (N=160). \textbf {Caution:} These results may not generalize to the full dataset}{table.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Full-Scale Results}{46}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Current State}{46}{subsection.4.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Sample vs Full-Scale Performance Comparison}}{46}{table.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Comparison: Sample vs. Full Dataset}{46}{subsection.4.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Comparison of sample-based tuning vs. earlier full-dataset runs}}{46}{table.4.3}\protected@file@percent }
\newlabel{tab:sample_vs_full}{{4.3}{46}{Comparison of sample-based tuning vs. earlier full-dataset runs}{table.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Training Dynamics}{46}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Loss Curves}{46}{subsection.4.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Training loss components for the Constrained Autoencoder (CAE) over 30 epochs. Blue: total loss ($\mathcal  {L}_{total}$), orange: reconstruction loss ($\mathcal  {L}_{recon}$), green: tag prediction loss ($\mathcal  {L}_{tag}$) with weight $\lambda _{tag} = 0.5$. All components show decreasing trends, converging by approximately epoch 20.}}{47}{figure.4.1}\protected@file@percent }
\newlabel{fig:training_loss_ae}{{4.1}{47}{Training loss components for the Constrained Autoencoder (CAE) over 30 epochs. Blue: total loss ($\mathcal {L}_{total}$), orange: reconstruction loss ($\mathcal {L}_{recon}$), green: tag prediction loss ($\mathcal {L}_{tag}$) with weight $\lambda _{tag} = 0.5$. All components show decreasing trends, converging by approximately epoch 20}{figure.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Training loss for Constrained Autoencoder (CAE). Multiple loss components are shown: total loss (blue), reconstruction loss, and tag prediction loss. All components show decreasing trends over 30 epochs. [to be updated]}}{48}{figure.4.2}\protected@file@percent }
\newlabel{fig:training_loss_cae}{{4.2}{48}{Training loss for Constrained Autoencoder (CAE). Multiple loss components are shown: total loss (blue), reconstruction loss, and tag prediction loss. All components show decreasing trends over 30 epochs. [to be updated]}{figure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Training loss for DECCS mode. \textbf  {Notable pattern:} Periodic spikes occur every 5 epochs, corresponding to when the consensus matrix is rebuilt. The spikes indicate that the new consensus targets temporarily increase the loss before the model adapts.}}{49}{figure.4.3}\protected@file@percent }
\newlabel{fig:training_loss_deccs}{{4.3}{49}{Training loss for DECCS mode. \textbf {Notable pattern:} Periodic spikes occur every 5 epochs, corresponding to when the consensus matrix is rebuilt. The spikes indicate that the new consensus targets temporarily increase the loss before the model adapts}{figure.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Embedding Space Visualization}{49}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}PCA Projections}{49}{subsection.4.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces PCA projection of baseline autoencoder embeddings. The distribution forms a dense elliptical core with a prominent tail of outliers extending along PC1 (teal points, reaching PC1$\approx $40). These outliers likely represent visually distinctive samples that the reconstruction objective maps to extreme embedding values. Cluster colors are heavily mixed within the core, indicating the reconstruction-only objective does not naturally separate semantic categories.}}{50}{figure.4.4}\protected@file@percent }
\newlabel{fig:pca_ae}{{4.4}{50}{PCA projection of baseline autoencoder embeddings. The distribution forms a dense elliptical core with a prominent tail of outliers extending along PC1 (teal points, reaching PC1$\approx $40). These outliers likely represent visually distinctive samples that the reconstruction objective maps to extreme embedding values. Cluster colors are heavily mixed within the core, indicating the reconstruction-only objective does not naturally separate semantic categories}{figure.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces PCA projection of Constrained Autoencoder (CAE) embeddings. Tag supervision reshapes the embedding geometry into an L-shaped distribution: a vertical arm extending upward (blue points, PC2$\approx $20) and a horizontal arm extending rightward (teal points, PC1$\approx $40). This bifurcation suggests the tag prediction branch creates distinct embedding regions for samples with different dominant attributes. However, cluster colors remain mixed in the dense core, indicating semantic supervision alone does not fully resolve cluster boundaries.}}{51}{figure.4.5}\protected@file@percent }
\newlabel{fig:pca_cae}{{4.5}{51}{PCA projection of Constrained Autoencoder (CAE) embeddings. Tag supervision reshapes the embedding geometry into an L-shaped distribution: a vertical arm extending upward (blue points, PC2$\approx $20) and a horizontal arm extending rightward (teal points, PC1$\approx $40). This bifurcation suggests the tag prediction branch creates distinct embedding regions for samples with different dominant attributes. However, cluster colors remain mixed in the dense core, indicating semantic supervision alone does not fully resolve cluster boundaries}{figure.4.5}{}}
\citation{vanderMaaten2014}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces PCA projection of DECCS embeddings. The consensus mechanism produces a curved, arc-shaped distribution spanning PC1$\in $[-12, 18] and PC2$\in $[-25, 7]. Unlike AE and CAE, some spatial organization by cluster is visible: green/yellow points concentrate on the right, blue points in the upper region, and purple outliers extend downward. The arc geometry suggests the consensus loss constrains embeddings to a lower-dimensional manifold while encouraging cluster separation.}}{52}{figure.4.6}\protected@file@percent }
\newlabel{fig:pca_deccs}{{4.6}{52}{PCA projection of DECCS embeddings. The consensus mechanism produces a curved, arc-shaped distribution spanning PC1$\in $[-12, 18] and PC2$\in $[-25, 7]. Unlike AE and CAE, some spatial organization by cluster is visible: green/yellow points concentrate on the right, blue points in the upper region, and purple outliers extend downward. The arc geometry suggests the consensus loss constrains embeddings to a lower-dimensional manifold while encouraging cluster separation}{figure.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}t-SNE Visualization}{53}{subsection.4.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces t-SNE projection of DECCS embeddings colored by cluster assignment (color bar indicates cluster IDs 0--30). The visualization reveals clear local structure: peripheral regions contain well-separated, single-color clusters (e.g., bright green at far left, orange at lower left, cyan at upper right), while the central region shows more mixing. This pattern is consistent with NMI=0.642---the clustering captures meaningful structure but does not perfectly align with all ground truth boundaries.}}{53}{figure.4.7}\protected@file@percent }
\newlabel{fig:tsne}{{4.7}{53}{t-SNE projection of DECCS embeddings colored by cluster assignment (color bar indicates cluster IDs 0--30). The visualization reveals clear local structure: peripheral regions contain well-separated, single-color clusters (e.g., bright green at far left, orange at lower left, cyan at upper right), while the central region shows more mixing. This pattern is consistent with NMI=0.642---the clustering captures meaningful structure but does not perfectly align with all ground truth boundaries}{figure.4.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Identified Challenges}{54}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Training Dynamics in DECCS Mode}{54}{subsection.4.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}ARI Near Zero}{54}{subsection.4.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}Sensitivity to Hyperparameters}{54}{subsection.4.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Summary of Results}{55}{section.4.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Summary of implementation status and results}}{55}{table.4.4}\protected@file@percent }
\newlabel{tab:status_summary}{{4.4}{55}{Summary of implementation status and results}{table.4.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Discussion}{56}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Overview}{56}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Interpretation of Results}{56}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Analysis of Scaling Failure}{56}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Consensus Matrix Scalability}{56}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Multi-Objective Optimization Challenges}{56}{subsection.5.2.1}\protected@file@percent }
\citation{Ruder2017,Crawshaw2020}
\citation{Ren2024,Zhou2024}
\@writefile{toc}{\contentsline {subsubsection}{Sample Representativeness}{57}{equation.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Comparison with DDC}{57}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Missing Pairwise Consistency Loss}{57}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Single Model vs. Ensemble Complexity}{57}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architecture Differences}{58}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Potential Issues to Investigate}{58}{subsection.5.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Metric Calculation}{58}{subsection.5.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training Dynamics}{58}{subsection.5.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Comparison with Research Hypotheses}{58}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Hypothesis 1: DDC Integration Improves Interpretability}{58}{subsection.5.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Hypothesis 2: Interpretability Does Not Compromise Performance}{58}{subsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Hypothesis 3: Consensus Clustering Enhances Robustness}{59}{subsection.5.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Limitations and Challenges}{59}{section.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Implementation Limitations}{59}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Incomplete DECCS Integration}{59}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Simple Architecture}{59}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Methodological Limitations}{59}{subsection.5.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dependence on Predefined Attributes}{59}{subsection.5.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Class-Level vs. Instance-Level Attributes}{60}{subsection.5.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Binary Clustering Evaluation}{60}{subsection.5.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Dataset-Specific Challenges}{60}{subsection.5.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Class Imbalance}{60}{subsection.5.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Attribute Noise}{60}{subsection.5.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Theoretical Implications}{60}{section.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Multi-Task Learning for Clustering}{60}{subsection.5.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}The Value of Human-Aligned Representations}{60}{subsection.5.5.2}\protected@file@percent }
\citation{Miklautz2021}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Practical Implications}{61}{section.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}When to Use This Approach}{61}{subsection.5.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}When Other Methods May Be Preferable}{61}{subsection.5.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}Deployment Considerations}{61}{subsection.5.6.3}\protected@file@percent }
\citation{Zhang2021}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Comparison with Related Work}{62}{section.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}Comparison with Original DECCS}{62}{subsection.5.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}Comparison with Original DDC}{62}{subsection.5.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Summary}{62}{section.5.8}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{64}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Summary of Work}{64}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}What Was Accomplished}{64}{subsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}What Did Not Work}{64}{subsection.6.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Summary of Results}}{64}{table.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Research Questions Answered}{64}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}RQ1: How can DDC be integrated into DECCS?}{65}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}RQ2: Impact on Clustering Performance}{65}{subsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}RQ3: Performance on AwA2 Dataset}{65}{subsection.6.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Lessons Learned}{65}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Full-Scale Validation is Essential}{65}{subsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Multi-Objective Optimization is Difficult}{65}{subsection.6.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Limitations}{66}{section.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Future Work}{66}{section.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Immediate Priorities}{66}{subsection.6.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Longer-Term Research}{66}{subsection.6.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Final Remarks}{67}{section.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix A}{68}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Hyperparameter Optimization: Grid Search Configuration}{68}{section.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Complete Results Table}{68}{section.A.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Complete hyperparameter grid search results for DECCS mode}}{68}{table.A.1}\protected@file@percent }
\newlabel{tab:hyperparam_full}{{A.1}{68}{Complete hyperparameter grid search results for DECCS mode}{table.A.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Analysis of Hyperparameter Effects}{68}{section.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3.1}Impact of Consensus Weight ($\lambda _{consensus}$)}{68}{subsection.A.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Effect of consensus weight on clustering performance (with $\lambda _{tag}=1.0$)}}{69}{figure.A.1}\protected@file@percent }
\newlabel{fig:lambda_consensus_effect}{{A.1}{69}{Effect of consensus weight on clustering performance (with $\lambda _{tag}=1.0$)}{figure.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3.2}Impact of Tag Supervision Weight ($\lambda _{tag}$)}{69}{subsection.A.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Effect of tag supervision weight on clustering performance (with $\lambda _{consensus}=0.2$)}}{69}{figure.A.2}\protected@file@percent }
\newlabel{fig:lambda_tag_effect}{{A.2}{69}{Effect of tag supervision weight on clustering performance (with $\lambda _{consensus}=0.2$)}{figure.A.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Best Configuration}{70}{section.A.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Convergence Analysis}{70}{section.A.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Training convergence with optimal hyperparameters}}{70}{figure.A.3}\protected@file@percent }
\newlabel{fig:convergence_best}{{A.3}{70}{Training convergence with optimal hyperparameters}{figure.A.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Appendix B}{71}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Additional Visualizations and Cluster Analysis: Detailed Cluster Descriptions}{71}{section.B.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.1}Complete Cluster Attribute Analysis}{71}{subsection.B.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Representative cluster characterizations with top-5 attributes}}{71}{table.B.1}\protected@file@percent }
\newlabel{tab:all_clusters}{{B.1}{71}{Representative cluster characterizations with top-5 attributes}{table.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.2}Cluster Purity Analysis}{71}{subsection.B.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Attribute Importance Analysis}{71}{section.B.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.1}Most Discriminative Attributes}{71}{subsection.B.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Top 15 most discriminative attributes}}{72}{table.B.2}\protected@file@percent }
\newlabel{tab:attr_importance}{{B.2}{72}{Top 15 most discriminative attributes}{table.B.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Implementation Details}{73}{section.B.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.1}Model Architecture Specifications}{73}{subsection.B.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.2}Training Configuration}{74}{subsection.B.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.3}{\ignorespaces Complete training configuration}}{74}{table.B.3}\protected@file@percent }
\newlabel{tab:training_config}{{B.3}{74}{Complete training configuration}{table.B.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.4}Dataset Statistics}{74}{section.B.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4.1}AwA2 Dataset Breakdown}{74}{subsection.B.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.4}{\ignorespaces AwA2 dataset statistics}}{74}{table.B.4}\protected@file@percent }
\newlabel{tab:dataset_stats}{{B.4}{74}{AwA2 dataset statistics}{table.B.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.5}Ensemble Clustering Details}{74}{section.B.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5.1}Base Clustering Algorithms}{74}{subsection.B.5.1}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{bibliography}
\@writefile{lot}{\contentsline {table}{\numberline {B.5}{\ignorespaces Ensemble clustering algorithms and configurations}}{75}{table.B.5}\protected@file@percent }
\newlabel{tab:ensemble_config}{{B.5}{75}{Ensemble clustering algorithms and configurations}{table.B.5}{}}
\bibcite{Akata2015}{{1}{2015}{{Akata et~al.}}{{Akata, Reed, Walter, Lee, and Schiele}}}
\bibcite{Akata2016}{{2}{2016}{{Akata et~al.}}{{Akata, Perronnin, Harchaoui, and Schmid}}}
\bibcite{Balachandran2009}{{3}{2009}{{Balachandran et~al.}}{{Balachandran, Deepak, and Khemani}}}
\bibcite{Bengio2013}{{4}{2013}{{Bengio et~al.}}{{Bengio, Courville, and Vincent}}}
\bibcite{Caron2018}{{5}{2018}{{Caron et~al.}}{{Caron, Bojanowski, Joulin, and Douze}}}
\bibcite{Caron2020}{{6}{2020}{{Caron et~al.}}{{Caron, Misra, Mairal, Goyal, Bojanowski, and Joulin}}}
\bibcite{Chang2017}{{7}{2017}{{Chang et~al.}}{{Chang, Wang, Meng, Xiang, and Pan}}}
\bibcite{Chen2020}{{8}{2020}{{Chen et~al.}}{{Chen, Kornblith, Norouzi, and Hinton}}}
\bibcite{Chen2021}{{9}{2021}{{Chen and He}}{{}}}
\bibcite{Chhajer2022}{{10}{2022}{{Chhajer and Moniri}}{{}}}
\bibcite{Crawshaw2020}{{11}{2020}{{Crawshaw}}{{}}}
\bibcite{davidson2018}{{12}{2018}{{Davidson et~al.}}{{Davidson, Gourru, and Ravi}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{76}{appendix*.4}\protected@file@percent }
\bibcite{Fred2005}{{13}{2005}{{Fred and Jain}}{{}}}
\bibcite{Grill2020}{{14}{2020}{{Grill et~al.}}{{Grill, Strub, Altch\'{e}, Tallec, Richemond, Buchatskaya, Doersch, Pires, Guo, Azar, Piot, Kavukcuoglu, Munos, and Valko}}}
\bibcite{Guidotti2018}{{15}{2018}{{Guidotti et~al.}}{{Guidotti, Monreale, Ruggieri, Turini, Giannotti, and Pedreschi}}}
\bibcite{Guo2017}{{16}{2017}{{Guo et~al.}}{{Guo, Gao, Liu, and Yin}}}
\bibcite{He2020}{{17}{2020}{{He et~al.}}{{He, Fan, Wu, Xie, and Girshick}}}
\bibcite{Hubert1985}{{18}{1985}{{Hubert and Arabie}}{{}}}
\bibcite{Jaiswal2021}{{19}{2021}{{Jaiswal et~al.}}{{Jaiswal, Babu, Zadeh, Banerjee, and Makedon}}}
\bibcite{Jiang2017}{{20}{2017}{{Jiang et~al.}}{{Jiang, Zheng, Tan, Tang, and Zhou}}}
\bibcite{Kingma2015}{{21}{2015}{{Kingma and Ba}}{{}}}
\bibcite{Kingma2014}{{22}{2014}{{Kingma and Welling}}{{}}}
\bibcite{Kuhn1955}{{23}{1955}{{Kuhn}}{{}}}
\bibcite{Lampert2014}{{24}{2014}{{Lampert et~al.}}{{Lampert, Nickisch, and Harmeling}}}
\bibcite{LeCun2015}{{25}{2015}{{LeCun et~al.}}{{LeCun, Bengio, and Hinton}}}
\bibcite{Li2021}{{26}{2021}{{Li et~al.}}{{Li, Hu, Liu, Peng, Zhou, and Peng}}}
\bibcite{Liu2022}{{27}{2022}{{Liu et~al.}}{{Liu, Wang, Liu, and Yu}}}
\bibcite{Mautz2020}{{28}{2020}{{Mautz et~al.}}{{Mautz, Plant, and Böhm}}}
\bibcite{Miklautz2021}{{29}{2021}{{Miklautz et~al.}}{{Miklautz, Bauer, Mautz, Tschiatschek, Böhm, and Plant}}}
\bibcite{Min2018}{{30}{2018}{{Min et~al.}}{{Min, Guo, Liu, Zhang, Cui, and Long}}}
\bibcite{Ozyegen2022}{{31}{2022}{{Ozyegen et~al.}}{{Ozyegen, Prayogo, Cevik, and Basar}}}
\bibcite{Paszke2019}{{32}{2019}{{Paszke et~al.}}{{Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Köpf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala}}}
\bibcite{Plant2011}{{33}{2011}{{Plant and Böhm}}{{}}}
\bibcite{Ren2024}{{34}{2024}{{Ren et~al.}}{{Ren, Pu, Yang, Xu, Li, Pu, Yu, and He}}}
\bibcite{Ribeiro2016}{{35}{2016}{{Ribeiro et~al.}}{{Ribeiro, Singh, and Guestrin}}}
\bibcite{Rishinanda2021}{{36}{2021}{{Rishinanda and Sebag}}{{}}}
\bibcite{Rousseeuw1987}{{37}{1987}{{Rousseeuw}}{{}}}
\bibcite{Ruder2017}{{38}{2017}{{Ruder}}{{}}}
\bibcite{Saisubramanian2019}{{39}{2019}{{Saisubramanian et~al.}}{{Saisubramanian, Galhotra, and Zilberstein}}}
\bibcite{Sambaturu2020}{{40}{2020}{{Sambaturu et~al.}}{{Sambaturu, Gupta, Davidson, Ravi, Vullikanti, and Warren}}}
\bibcite{Shen2021}{{41}{2021}{{Shen et~al.}}{{Shen, Shen, Wang, Qin, Torr, and Shao}}}
\bibcite{Strehl2002}{{42}{2002}{{Strehl and Ghosh}}{{}}}
\bibcite{Tjoa2023}{{43}{2021}{{Tjoa and Guan}}{{}}}
\bibcite{vanderMaaten2014}{{44}{2014}{{van~der Maaten}}{{}}}
\bibcite{Vega-Pons2011}{{45}{2011}{{Vega-Pons and Ruiz-Shulcloper}}{{}}}
\bibcite{Vinh2010}{{46}{2010}{{Vinh et~al.}}{{Vinh, Epps, and Bailey}}}
\bibcite{Xian2019}{{47}{2019}{{Xian et~al.}}{{Xian, Lampert, Schiele, and Akata}}}
\bibcite{Xie2016}{{48}{2016}{{Xie et~al.}}{{Xie, Girshick, and Farhadi}}}
\bibcite{Yang2017}{{49}{2017}{{Yang et~al.}}{{Yang, Fu, Sidiropoulos, and Hong}}}
\bibcite{Yang2016}{{50}{2016}{{Yang et~al.}}{{Yang, Parikh, and Batra}}}
\bibcite{Zhang2021}{{51}{2021}{{Zhang and Davidson}}{{}}}
\bibcite{Zhong2021}{{52}{2021}{{Zhong et~al.}}{{Zhong, Wu, Chen, Huang, Deng, Nie, Lin, and Hua}}}
\bibcite{Zhou2024}{{53}{2024}{{Zhou et~al.}}{{Zhou, Xu, Zheng, Chen, Li, Bu, Wu, Wang, Zhu, and Ester}}}
\gdef \@abspage@last{82}
