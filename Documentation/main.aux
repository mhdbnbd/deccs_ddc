\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Abstract}{1}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Contents}{1}{chapter*.1}\protected@file@percent }
\citation{Miklautz2021}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{6}{chapter*.2}\protected@file@percent }
\citation{Miklautz2021}
\citation{Zhang2021}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{8}{chapter*.3}\protected@file@percent }
\citation{LeCun2015}
\citation{Ren2024}
\citation{Guidotti2018}
\citation{Saisubramanian2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{9}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Background}{9}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Problem Statement}{9}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Formal Problem Definition}{9}{subsection.1.2.1}\protected@file@percent }
\citation{Ruder2017,Crawshaw2020}
\citation{Bengio2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Key Challenges}{10}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Research Objectives}{10}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Contributions}{11}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Methodological Contributions}{11}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Implementation Contributions}{11}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Thesis Structure}{12}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Scope and Delimitations}{12}{section.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}In Scope}{12}{subsection.1.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Out of Scope}{13}{subsection.1.6.2}\protected@file@percent }
\citation{Miklautz2021}
\citation{Min2018,Zhou2024}
\citation{Vega-Pons2011}
\citation{Strehl2002,Fred2005}
\citation{Miklautz2021}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{14}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Deep Embedded Clustering with Consensus Representations (DECCS)}{14}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Related Work in Consensus Clustering and Deep Clustering}{14}{subsection.2.1.1}\protected@file@percent }
\citation{Xie2016}
\citation{Guo2017}
\citation{Jiang2017}
\citation{Chang2017}
\citation{Yang2016}
\citation{Caron2018}
\citation{Mautz2020}
\citation{Kingma2014}
\citation{Miklautz2021}
\citation{Miklautz2021}
\citation{Vinh2010}
\citation{Zhang2021}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Visualisation of one round of DECCS\citep  {Miklautz2021}. (1) The encoder is used to embed data points $X$. (2) Clustering results are generated by applying ensemble members $\mathcal  {E} = \{KM, \ldots  , SC\}$ to $Z$. (3) Classifiers $g_i$ are trained to predict the corresponding cluster labels $\pi _i$ from $Z$. (4) $Z$ is updated via minimizing $\mathcal  {L}$.}}{16}{figure.2.1}\protected@file@percent }
\newlabel{fig:pipeline}{{2.1}{16}{Visualisation of one round of DECCS\citep {Miklautz2021}. (1) The encoder is used to embed data points $X$. (2) Clustering results are generated by applying ensemble members $\mathcal {E} = \{KM, \ldots , SC\}$ to $Z$. (3) Classifiers $g_i$ are trained to predict the corresponding cluster labels $\pi _i$ from $Z$. (4) $Z$ is updated via minimizing $\mathcal {L}$}{figure.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Methodology of DECCS}{16}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Experimental Evaluation}{16}{subsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The framework of deep descriptive clustering (DDC). DDC consists of one clustering objective, one sub-symbolic explanation objective, and one self-generated objective to maximize the consistency between clustering and explanation modules.}}{17}{figure.2.2}\protected@file@percent }
\newlabel{fig:ddc}{{2.2}{17}{The framework of deep descriptive clustering (DDC). DDC consists of one clustering objective, one sub-symbolic explanation objective, and one self-generated objective to maximize the consistency between clustering and explanation modules}{figure.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Deep Descriptive Clustering (DDC)}{17}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Key Aspects of Deep Descriptive Clustering (DDC)}{17}{subsection.2.2.1}\protected@file@percent }
\citation{Xie2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Experiments and Findings}{18}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Conclusion and Future Directions}{18}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Related Works}{18}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Deep Embedded Clustering: A General Approach to Clustering Arbitrary Similarity Measures by J. Xie, R. Girshick, and A. Farhadi (2016)}{18}{subsection.2.3.1}\protected@file@percent }
\citation{Kingma2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Architecture of the Deep Embedded Clustering (DEC) model. The model consists of a stacked autoencoder followed by a clustering layer. The autoencoder learns a low-dimensional representation of the data, which is then clustered using a clustering objective.}}{19}{figure.2.3}\protected@file@percent }
\newlabel{fig:dec_architecture}{{2.3}{19}{Architecture of the Deep Embedded Clustering (DEC) model. The model consists of a stacked autoencoder followed by a clustering layer. The autoencoder learns a low-dimensional representation of the data, which is then clustered using a clustering objective}{figure.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Auto-Encoding Variational Bayes by D. P. Kingma and M. Welling (2014)}{19}{subsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Architecture of the Variational Autoencoder (VAE) model. The encoder maps input data to a latent space, and the decoder reconstructs the data from the latent space. The model is trained to minimize reconstruction loss and regularization loss.}}{19}{figure.2.4}\protected@file@percent }
\newlabel{fig:vae_architecture}{{2.4}{19}{Architecture of the Variational Autoencoder (VAE) model. The encoder maps input data to a latent space, and the decoder reconstructs the data from the latent space. The model is trained to minimize reconstruction loss and regularization loss}{figure.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Explainable-by-Design Algorithms}{19}{subsection.2.3.3}\protected@file@percent }
\citation{Sambaturu2020}
\citation{Kauffmann2022}
\citation{Chen2020}
\citation{He2020}
\citation{Caron2020}
\citation{Grill2020}
\citation{Li2021}
\citation{Zhong2021}
\citation{Shen2021}
\citation{Jaiswal2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Post-processing Explanation Methods}{20}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Other Related Works}{20}{subsection.2.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Comprehensive Comparison of Clustering Approaches}{21}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Taxonomy of Deep Clustering Methods}{21}{subsection.2.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Taxonomy of deep clustering approaches}}{21}{table.2.1}\protected@file@percent }
\newlabel{tab:taxonomy}{{2.1}{21}{Taxonomy of deep clustering approaches}{table.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Detailed Method Comparison}{21}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Deep Embedded Clustering (DEC)}{21}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Variational Autoencoders for Clustering (VAE-based)}{22}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Deep Clustering with Consensus Representations (DECCS)}{22}{subsection.2.4.2}\protected@file@percent }
\citation{Xie2016}
\citation{Miklautz2021}
\citation{Zhang2021}
\@writefile{toc}{\contentsline {subsubsection}{Deep Descriptive Clustering (DDC)}{23}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Quantitative Performance Comparison}{23}{subsection.2.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Performance comparison of deep clustering methods on benchmark datasets}}{24}{table.2.2}\protected@file@percent }
\newlabel{tab:methods_comparison}{{2.2}{24}{Performance comparison of deep clustering methods on benchmark datasets}{table.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Qualitative Feature Comparison}{24}{subsection.2.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Qualitative comparison of clustering method features}}{24}{table.2.3}\protected@file@percent }
\newlabel{tab:qualitative_comparison}{{2.3}{24}{Qualitative comparison of clustering method features}{table.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Computational Complexity Analysis}{24}{subsection.2.4.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Computational complexity comparison}}{24}{table.2.4}\protected@file@percent }
\newlabel{tab:complexity}{{2.4}{24}{Computational complexity comparison}{table.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.6}Applicability Analysis}{25}{subsection.2.4.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces Method applicability to different data types and scenarios}}{25}{table.2.5}\protected@file@percent }
\newlabel{tab:applicability}{{2.5}{25}{Method applicability to different data types and scenarios}{table.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.7}Key Innovations by Method}{25}{subsection.2.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.8}Limitations Comparison}{26}{subsection.2.4.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.6}{\ignorespaces Method limitations and failure modes}}{26}{table.2.6}\protected@file@percent }
\newlabel{tab:limitations}{{2.6}{26}{Method limitations and failure modes}{table.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Evolution of Deep Clustering Paradigms}{26}{section.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Evolution and relationships between deep clustering methods}}{26}{figure.2.5}\protected@file@percent }
\newlabel{fig:evolution}{{2.5}{26}{Evolution and relationships between deep clustering methods}{figure.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Research Gaps Addressed}{26}{subsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Positioning in the Literature}{27}{subsection.2.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Our work at the intersection of three research areas}}{27}{figure.2.6}\protected@file@percent }
\newlabel{fig:venn}{{2.6}{27}{Our work at the intersection of three research areas}{figure.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Summary and Implications for This Research}{27}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Key Insights from the Literature}{28}{subsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Research Gap}{28}{subsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Positioning of Our Work}{28}{subsection.2.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{30}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Overview}{30}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Methodological Contributions}{30}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Research Design}{30}{subsection.3.1.2}\protected@file@percent }
\citation{Xian2019,Lampert2014}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Overview of our integrated methodology combining semantic supervision, representation learning, and consensus clustering}}{31}{figure.3.1}\protected@file@percent }
\newlabel{fig:methodology_overview}{{3.1}{31}{Overview of our integrated methodology combining semantic supervision, representation learning, and consensus clustering}{figure.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Dataset Preparation}{31}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Animals with Attributes 2 (AwA2) Dataset}{31}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Semantic Attribute Structure}{31}{subsection.3.2.2}\protected@file@percent }
\citation{Paszke2019}
\citation{Kingma2015}
\citation{Ioffe2015}
\citation{He2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Data Preprocessing Pipeline}{33}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Image Preprocessing}{33}{subsection.3.2.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Image Preprocessing Pipeline}}{33}{algorithm.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Attribute Normalization}{33}{ALC@unique.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dataset Splitting}{33}{equation.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Data Loading Implementation}{33}{subsection.3.2.4}\protected@file@percent }
\citation{Bengio2013}
\citation{Akata2015,Akata2016}
\citation{Chen2021}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Model Architecture}{34}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Baseline Autoencoder (AE)}{34}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Encoder Architecture}{34}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Decoder Architecture}{34}{equation.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Baseline Loss Function}{34}{equation.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Constrained Autoencoder (CAE)}{34}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architecture Extension}{35}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Multi-Objective Loss Function}{35}{equation.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Hyperparameter Selection}{35}{equation.3.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Consensus Clustering Integration (DECCS)}{35}{subsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Consensus Representation Learning}{35}{subsection.3.3.3}\protected@file@percent }
\citation{Yang2017}
\@writefile{toc}{\contentsline {subsubsection}{Ensemble Clustering Algorithms}{36}{equation.3.11}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Ensemble clustering algorithms and their characteristics}}{36}{table.3.1}\protected@file@percent }
\newlabel{tab:ensemble_algorithms}{{3.1}{36}{Ensemble clustering algorithms and their characteristics}{table.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Consensus Matrix Construction}{36}{table.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Consensus Consistency Loss}{36}{equation.3.13}\protected@file@percent }
\citation{Kingma2015}
\@writefile{toc}{\contentsline {subsubsection}{Full DECCS Loss}{37}{equation.3.15}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Training Procedure}{37}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Optimization Strategy}{37}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Optimizer Configuration}{37}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Batch Processing}{37}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Mixed Precision Training}{37}{subsection.3.4.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Mixed Precision Training Step}}{37}{algorithm.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Training Algorithms}{38}{subsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Baseline Autoencoder Training}{38}{subsection.3.4.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Train Baseline Autoencoder}}{38}{algorithm.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Constrained Autoencoder Training}{38}{ALC@unique.20}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Train Constrained Autoencoder}}{38}{algorithm.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{DECCS Training with Consensus}{39}{ALC@unique.32}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Train DECCS with Consensus}}{39}{algorithm.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Clustering and Evaluation}{39}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Embedding Extraction}{39}{subsection.3.5.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Extract Embeddings}}{39}{algorithm.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Consensus Clustering}{39}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Ensemble Execution}{39}{subsection.3.5.2}\protected@file@percent }
\citation{Rousseeuw1987}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Consensus Clustering Evaluation}}{40}{algorithm.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Evaluation Metrics}{40}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Clustering Performance Metrics}{40}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Normalized Mutual Information (NMI)}{40}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Adjusted Rand Index (ARI)}{40}{equation.3.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Clustering Accuracy (ACC)}{40}{equation.3.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Silhouette Score}{40}{equation.3.19}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Cluster Explanation Generation}{41}{section.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Attribute-Based Characterization}{41}{subsection.3.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Top-K Attribute Selection}{41}{subsection.3.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Explanation Quality Metrics}{41}{subsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Cluster Purity}{41}{subsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Attribute Discriminativeness}{41}{equation.3.23}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Implementation Details}{41}{section.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Software Framework}{41}{subsection.3.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Hardware Configuration}{41}{subsection.3.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Reproducibility}{42}{subsection.3.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Limitations and Design Choices}{42}{section.3.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Methodological Limitations}{42}{subsection.3.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}Design Rationale}{42}{subsection.3.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Hyperparameter Selection}{43}{section.3.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.1}Grid Search on Sample Subset}{43}{subsection.3.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.2}Tuning Results Summary}{43}{subsection.3.9.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Hyperparameter tuning results on sample subset (N=160)}}{43}{table.3.2}\protected@file@percent }
\newlabel{tab:hyperparam_tuning}{{3.2}{43}{Hyperparameter tuning results on sample subset (N=160)}{table.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.10}Summary of Methodological Choices}{44}{section.3.10}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Final configuration summary}}{44}{table.3.3}\protected@file@percent }
\newlabel{tab:table}{{3.3}{44}{Final configuration summary}{table.3.3}{}}
\citation{Xian2019}
\citation{Kingma2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{45}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experimental Setup}{45}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Hyperparameters}{45}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Hyperparameter Tuning on Sample Subset}{45}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Grid Search Configuration}{46}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Sample-Based Tuning Results}{46}{subsection.4.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Hyperparameter tuning results on sample subset (N=160). \textbf  {Caution:} These results may not generalize to the full dataset.}}{46}{table.4.1}\protected@file@percent }
\newlabel{tab:tuning_results}{{4.1}{46}{Hyperparameter tuning results on sample subset (N=160). \textbf {Caution:} These results may not generalize to the full dataset}{table.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Full-Scale Results}{47}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Current State}{47}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Comparison: Sample vs. Full Dataset}{47}{subsection.4.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Comparison of sample-based tuning vs. earlier full-dataset runs}}{47}{table.4.2}\protected@file@percent }
\newlabel{tab:sample_vs_full}{{4.2}{47}{Comparison of sample-based tuning vs. earlier full-dataset runs}{table.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Training Dynamics}{47}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Loss Curves}{47}{subsection.4.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Training loss for baseline Autoencoder (AE). The reconstruction loss decreases smoothly from approximately 0.032 to 0.006 over 30 epochs, indicating successful convergence of the reconstruction objective.}}{48}{figure.4.1}\protected@file@percent }
\newlabel{fig:training_loss_ae}{{4.1}{48}{Training loss for baseline Autoencoder (AE). The reconstruction loss decreases smoothly from approximately 0.032 to 0.006 over 30 epochs, indicating successful convergence of the reconstruction objective}{figure.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Training loss for Constrained Autoencoder (CAE). Multiple loss components are shown: total loss (blue), reconstruction loss, and tag prediction loss. All components show decreasing trends over 30 epochs.}}{49}{figure.4.2}\protected@file@percent }
\newlabel{fig:training_loss_cae}{{4.2}{49}{Training loss for Constrained Autoencoder (CAE). Multiple loss components are shown: total loss (blue), reconstruction loss, and tag prediction loss. All components show decreasing trends over 30 epochs}{figure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Training loss for DECCS mode. \textbf  {Notable pattern:} Periodic spikes occur every 5 epochs, corresponding to when the consensus matrix is rebuilt. The spikes indicate that the new consensus targets temporarily increase the loss before the model adapts.}}{50}{figure.4.3}\protected@file@percent }
\newlabel{fig:training_loss_deccs}{{4.3}{50}{Training loss for DECCS mode. \textbf {Notable pattern:} Periodic spikes occur every 5 epochs, corresponding to when the consensus matrix is rebuilt. The spikes indicate that the new consensus targets temporarily increase the loss before the model adapts}{figure.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Embedding Space Visualization}{50}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}PCA Projections}{50}{subsection.4.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces PCA projection of baseline AE embeddings. Points are colored by cluster assignment. The embedding space shows some structure with outliers on the right side.}}{51}{figure.4.4}\protected@file@percent }
\newlabel{fig:pca_ae}{{4.4}{51}{PCA projection of baseline AE embeddings. Points are colored by cluster assignment. The embedding space shows some structure with outliers on the right side}{figure.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces PCA projection of CAE embeddings. The distribution shows a similar pattern to AE with a concentrated core and extended outliers.}}{51}{figure.4.5}\protected@file@percent }
\newlabel{fig:pca_cae}{{4.5}{51}{PCA projection of CAE embeddings. The distribution shows a similar pattern to AE with a concentrated core and extended outliers}{figure.4.5}{}}
\citation{vanderMaaten2014}
\citation{McInnes2018}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces PCA projection of DECCS embeddings. The embedding space shows a roughly arc-shaped distribution, with colors representing cluster assignments that do not show clear separation.}}{52}{figure.4.6}\protected@file@percent }
\newlabel{fig:pca_deccs}{{4.6}{52}{PCA projection of DECCS embeddings. The embedding space shows a roughly arc-shaped distribution, with colors representing cluster assignments that do not show clear separation}{figure.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}t-SNE Visualization}{52}{subsection.4.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces t-SNE projection of DECCS embeddings colored by cluster assignment. Local structure is visible with groups of same-colored points forming coherent regions, particularly in peripheral areas.}}{53}{figure.4.7}\protected@file@percent }
\newlabel{fig:tsne}{{4.7}{53}{t-SNE projection of DECCS embeddings colored by cluster assignment. Local structure is visible with groups of same-colored points forming coherent regions, particularly in peripheral areas}{figure.4.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Identified Challenges}{53}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Training Dynamics in DECCS Mode}{53}{subsection.4.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}ARI Near Zero}{54}{subsection.4.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}Sensitivity to Hyperparameters}{54}{subsection.4.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Summary of Results}{54}{section.4.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Summary of implementation status and results}}{54}{table.4.3}\protected@file@percent }
\newlabel{tab:status_summary}{{4.3}{54}{Summary of implementation status and results}{table.4.3}{}}
\citation{Miklautz2021}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Discussion}{56}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Overview}{56}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Analysis of Sample-Based Tuning Results}{56}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Promising Metrics on Small Sample}{56}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Why Did Earlier Full-Scale Runs Fail?}{56}{subsection.5.2.2}\protected@file@percent }
\citation{Bengio2013}
\citation{Kingma2014}
\citation{Ruder2017,Crawshaw2020}
\citation{Miklautz2021}
\citation{Zhang2021}
\citation{Ren2024,Zhou2024}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}The Central Question}{57}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Relationship to Prior Work}{57}{section.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Analysis of Training Dynamics}{57}{section.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Loss Curve Evidence}{57}{subsection.5.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Hypothesis: Scale-Dependent Instability}{58}{subsection.5.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Research Questions: Partial Answers}{58}{section.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}RQ1: How can DDC be integrated into DECCS?}{58}{subsection.5.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}RQ2: Impact on Clustering Performance}{58}{subsection.5.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}RQ3: Performance on AwA2 Dataset}{58}{subsection.5.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Lessons Learned}{58}{section.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}Hyperparameter Sensitivity}{58}{subsection.5.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}The Importance of Scale}{59}{subsection.5.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.3}Honest Reporting}{59}{subsection.5.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Proposed Path Forward}{59}{section.5.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.1}Immediate Priority}{59}{subsection.5.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.2}If Full-Scale Results Are Good}{59}{subsection.5.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.3}If Full-Scale Results Are Poor}{59}{subsection.5.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Summary}{60}{section.5.9}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{61}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Summary of Work Completed}{61}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Implementation Contributions}{61}{subsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Current Status: Honest Assessment}{62}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}What We Know}{62}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}What We Don't Know}{62}{subsection.6.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Summary of experimental results}}{62}{table.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Research Questions: Status}{62}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}RQ1: How can DDC be integrated into DECCS?}{62}{subsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}RQ2: Impact on Clustering Performance}{63}{subsection.6.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}RQ3: Performance on AwA2 Dataset}{63}{subsection.6.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Concrete Next Steps}{63}{section.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Critical: Full-Scale Validation}{63}{subsection.6.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}If Results Are Good}{63}{subsection.6.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}If Results Are Poor}{63}{subsection.6.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Lessons Learned}{64}{section.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}On Experimental Methodology}{64}{subsection.6.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}On Honest Reporting}{64}{subsection.6.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Broader Contributions}{64}{section.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Final Remarks}{64}{section.6.7}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix A: Implementation Details}{66}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Code Architecture}{66}{section.A.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Codebase structure}}{66}{table.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Hyperparameter Tuning Script}{66}{section.A.2}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.1}Hyperparameter tuning script structure}{66}{lstlisting.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Model Architecture Details}{67}{section.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3.1}Autoencoder Architecture}{67}{subsection.A.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3.2}Parameter Count}{68}{subsection.A.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Training Configuration}{68}{section.A.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Training hyperparameters used in experiments}}{68}{table.A.2}\protected@file@percent }
\newlabel{tab:training_config}{{A.2}{68}{Training hyperparameters used in experiments}{table.A.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Loss Configuration}{68}{section.A.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces Loss weights used in experiments}}{68}{table.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.6}Ensemble Clustering Configuration}{69}{section.A.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {A.4}{\ignorespaces Base clustering algorithms in ensemble}}{69}{table.A.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.7}Consensus Matrix Construction}{69}{section.A.7}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces build\_sparse\_consensus()}}{69}{algorithm.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.8}Command Line Interface}{69}{section.A.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.9}Output Files}{70}{section.A.9}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {A.5}{\ignorespaces Output files generated by experiments}}{70}{table.A.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.10}Reproducibility}{70}{section.A.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.11}Known Issues and Workarounds}{71}{section.A.11}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Appendix B: Visualizations}{72}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Embedding Visualizations}{72}{section.B.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.1}PCA Projections}{72}{subsection.B.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces PCA projection of baseline Autoencoder (AE) embeddings. Colors represent cluster assignments from K-Means. The distribution shows a concentrated core with outlier points extending to the right.}}{72}{figure.B.1}\protected@file@percent }
\newlabel{fig:pca_ae_app}{{B.1}{72}{PCA projection of baseline Autoencoder (AE) embeddings. Colors represent cluster assignments from K-Means. The distribution shows a concentrated core with outlier points extending to the right}{figure.B.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces PCA projection of Constrained Autoencoder (CAE) embeddings. Similar structure to AE with a dense core and extended outliers, suggesting tag supervision did not fundamentally change the embedding geometry.}}{73}{figure.B.2}\protected@file@percent }
\newlabel{fig:pca_cae_app}{{B.2}{73}{PCA projection of Constrained Autoencoder (CAE) embeddings. Similar structure to AE with a dense core and extended outliers, suggesting tag supervision did not fundamentally change the embedding geometry}{figure.B.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces PCA projection of DECCS embeddings. The distribution forms a roughly arc-shaped pattern. Color gradients do not correspond to spatially distinct clusters.}}{74}{figure.B.3}\protected@file@percent }
\newlabel{fig:pca_deccs_app}{{B.3}{74}{PCA projection of DECCS embeddings. The distribution forms a roughly arc-shaped pattern. Color gradients do not correspond to spatially distinct clusters}{figure.B.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.2}t-SNE Visualization}{75}{subsection.B.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces t-SNE projection of DECCS embeddings. Multiple distinct clusters are visible (see colorbar), with local structure showing groups of same-colored points forming coherent regions.}}{75}{figure.B.4}\protected@file@percent }
\newlabel{fig:tsne_app}{{B.4}{75}{t-SNE projection of DECCS embeddings. Multiple distinct clusters are visible (see colorbar), with local structure showing groups of same-colored points forming coherent regions}{figure.B.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Training Loss Curves}{76}{section.B.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.1}Baseline Autoencoder}{76}{subsection.B.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {B.5}{\ignorespaces Training loss for baseline Autoencoder over 30 epochs. Reconstruction loss (MSE) decreases smoothly from approximately 0.032 to 0.006.}}{76}{figure.B.5}\protected@file@percent }
\newlabel{fig:loss_ae_app}{{B.5}{76}{Training loss for baseline Autoencoder over 30 epochs. Reconstruction loss (MSE) decreases smoothly from approximately 0.032 to 0.006}{figure.B.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.2}Constrained Autoencoder}{77}{subsection.B.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {B.6}{\ignorespaces Training loss for Constrained Autoencoder. Multiple curves show total loss, reconstruction loss, and tag prediction loss components.}}{77}{figure.B.6}\protected@file@percent }
\newlabel{fig:loss_cae_app}{{B.6}{77}{Training loss for Constrained Autoencoder. Multiple curves show total loss, reconstruction loss, and tag prediction loss components}{figure.B.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.3}DECCS Mode}{78}{subsection.B.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {B.7}{\ignorespaces Training loss for DECCS mode showing characteristic periodic spikes every 5 epochs corresponding to consensus matrix rebuilding.}}{78}{figure.B.7}\protected@file@percent }
\newlabel{fig:loss_deccs_app}{{B.7}{78}{Training loss for DECCS mode showing characteristic periodic spikes every 5 epochs corresponding to consensus matrix rebuilding}{figure.B.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}AwA2 Dataset Statistics}{79}{section.B.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.1}Dataset Overview}{79}{subsection.B.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces AwA2 dataset statistics}}{79}{table.B.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.2}Attribute Categories}{79}{subsection.B.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Attribute categories in AwA2}}{79}{table.B.2}\protected@file@percent }
\citation{Miklautz2021}
\citation{Miklautz2021}
\citation{Zhang2021}
\citation{Zhang2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.3}Sample Classes}{80}{subsection.B.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.3}{\ignorespaces Sample of AwA2 classes}}{80}{table.B.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B.4}Framework Diagrams}{80}{section.B.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4.1}DECCS Framework}{80}{subsection.B.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {B.8}{\ignorespaces DECCS framework from \cite  {Miklautz2021}. (1) Encoder embeds input data. (2) Ensemble clustering algorithms generate base partitions. (3) Classifiers approximate partitions. (4) Representation is updated via consensus loss.}}{80}{figure.B.8}\protected@file@percent }
\newlabel{fig:deccs_framework}{{B.8}{80}{DECCS framework from \cite {Miklautz2021}. (1) Encoder embeds input data. (2) Ensemble clustering algorithms generate base partitions. (3) Classifiers approximate partitions. (4) Representation is updated via consensus loss}{figure.B.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4.2}DDC Framework}{81}{subsection.B.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {B.9}{\ignorespaces Deep Descriptive Clustering (DDC) framework from \cite  {Zhang2021}. Combines clustering objective with class-level explanations via Integer Linear Programming.}}{81}{figure.B.9}\protected@file@percent }
\newlabel{fig:ddc_framework}{{B.9}{81}{Deep Descriptive Clustering (DDC) framework from \cite {Zhang2021}. Combines clustering objective with class-level explanations via Integer Linear Programming}{figure.B.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.5}Placeholder: Results to Add After Debugging}{81}{section.B.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5.1}Cluster Descriptions}{81}{subsection.B.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.4}{\ignorespaces Placeholder: Cluster attribute descriptions}}{81}{table.B.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5.2}Cluster Purity Analysis}{81}{subsection.B.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.5}{\ignorespaces Placeholder: Cluster purity metrics}}{81}{table.B.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5.3}Confusion Matrix}{81}{subsection.B.5.3}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{bibliography}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5.4}Sample Images per Cluster}{82}{subsection.B.5.4}\protected@file@percent }
\bibcite{Akata2015}{{1}{2015}{{Akata et~al.}}{{Akata, Reed, Walter, Lee, and Schiele}}}
\bibcite{Akata2016}{{2}{2016}{{Akata et~al.}}{{Akata, Perronnin, Harchaoui, and Schmid}}}
\bibcite{Bengio2013}{{3}{2013}{{Bengio et~al.}}{{Bengio, Courville, and Vincent}}}
\bibcite{Caron2018}{{4}{2018}{{Caron et~al.}}{{Caron, Bojanowski, Joulin, and Douze}}}
\bibcite{Caron2020}{{5}{2020}{{Caron et~al.}}{{Caron, Misra, Mairal, Goyal, Bojanowski, and Joulin}}}
\bibcite{Chang2017}{{6}{2017}{{Chang et~al.}}{{Chang, Wang, Meng, Xiang, and Pan}}}
\bibcite{Chen2020}{{7}{2020}{{Chen et~al.}}{{Chen, Kornblith, Norouzi, and Hinton}}}
\bibcite{Chen2021}{{8}{2021}{{Chen and He}}{{}}}
\bibcite{Crawshaw2020}{{9}{2020}{{Crawshaw}}{{}}}
\bibcite{Fred2005}{{10}{2005}{{Fred and Jain}}{{}}}
\bibcite{Grill2020}{{11}{2020}{{Grill et~al.}}{{Grill, Strub, Altch\'{e}, Tallec, Richemond, Buchatskaya, Doersch, Pires, Guo, Azar, Piot, Kavukcuoglu, Munos, and Valko}}}
\bibcite{Guidotti2018}{{12}{2018}{{Guidotti et~al.}}{{Guidotti, Monreale, Ruggieri, Turini, Giannotti, and Pedreschi}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{83}{appendix*.4}\protected@file@percent }
\bibcite{Guo2017}{{13}{2017}{{Guo et~al.}}{{Guo, Gao, Liu, and Yin}}}
\bibcite{He2016}{{14}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{He2020}{{15}{2020}{{He et~al.}}{{He, Fan, Wu, Xie, and Girshick}}}
\bibcite{Ioffe2015}{{16}{2015}{{Ioffe and Szegedy}}{{}}}
\bibcite{Jaiswal2021}{{17}{2021}{{Jaiswal et~al.}}{{Jaiswal, Babu, Zadeh, Banerjee, and Makedon}}}
\bibcite{Jiang2017}{{18}{2017}{{Jiang et~al.}}{{Jiang, Zheng, Tan, Tang, and Zhou}}}
\bibcite{Kauffmann2022}{{19}{2022}{{Kauffmann et~al.}}{{Kauffmann, Esders, Montavon, Samek, and MÃ¼ller}}}
\bibcite{Kingma2015}{{20}{2015}{{Kingma and Ba}}{{}}}
\bibcite{Kingma2014}{{21}{2014}{{Kingma and Welling}}{{}}}
\bibcite{Lampert2014}{{22}{2014}{{Lampert et~al.}}{{Lampert, Nickisch, and Harmeling}}}
\bibcite{LeCun2015}{{23}{2015}{{LeCun et~al.}}{{LeCun, Bengio, and Hinton}}}
\bibcite{Li2021}{{24}{2021}{{Li et~al.}}{{Li, Hu, Liu, Peng, Zhou, and Peng}}}
\bibcite{Mautz2020}{{25}{2020}{{Mautz et~al.}}{{Mautz, Plant, and BÃ¶hm}}}
\bibcite{McInnes2018}{{26}{2018}{{McInnes et~al.}}{{McInnes, Healy, and Melville}}}
\bibcite{Miklautz2021}{{27}{2021}{{Miklautz et~al.}}{{Miklautz, Bauer, Mautz, Tschiatschek, BÃ¶hm, and Plant}}}
\bibcite{Min2018}{{28}{2018}{{Min et~al.}}{{Min, Guo, Liu, Zhang, Cui, and Long}}}
\bibcite{Paszke2019}{{29}{2019}{{Paszke et~al.}}{{Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, KÃ¶pf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala}}}
\bibcite{Ren2024}{{30}{2024}{{Ren et~al.}}{{Ren, Pu, Yang, Xu, Li, Pu, Yu, and He}}}
\bibcite{Rousseeuw1987}{{31}{1987}{{Rousseeuw}}{{}}}
\bibcite{Ruder2017}{{32}{2017}{{Ruder}}{{}}}
\bibcite{Saisubramanian2020}{{33}{2020}{{Saisubramanian et~al.}}{{Saisubramanian, Galhotra, and Zilberstein}}}
\bibcite{Sambaturu2020}{{34}{2020}{{Sambaturu et~al.}}{{Sambaturu, Gupta, Davidson, Ravi, Vullikanti, and Warren}}}
\bibcite{Shen2021}{{35}{2021}{{Shen et~al.}}{{Shen, Shen, Wang, Qin, Torr, and Shao}}}
\bibcite{Strehl2002}{{36}{2002}{{Strehl and Ghosh}}{{}}}
\bibcite{vanderMaaten2014}{{37}{2014}{{van~der Maaten}}{{}}}
\bibcite{Vega-Pons2011}{{38}{2011}{{Vega-Pons and Ruiz-Shulcloper}}{{}}}
\bibcite{Vinh2010}{{39}{2010}{{Vinh et~al.}}{{Vinh, Epps, and Bailey}}}
\bibcite{Xian2019}{{40}{2019}{{Xian et~al.}}{{Xian, Lampert, Schiele, and Akata}}}
\bibcite{Xie2016}{{41}{2016}{{Xie et~al.}}{{Xie, Girshick, and Farhadi}}}
\bibcite{Yang2017}{{42}{2017}{{Yang et~al.}}{{Yang, Fu, Sidiropoulos, and Hong}}}
\bibcite{Yang2016}{{43}{2016}{{Yang et~al.}}{{Yang, Parikh, and Batra}}}
\bibcite{Zhang2021}{{44}{2021}{{Zhang and Davidson}}{{}}}
\bibcite{Zhong2021}{{45}{2021}{{Zhong et~al.}}{{Zhong, Wu, Chen, Huang, Deng, Nie, Lin, and Hua}}}
\bibcite{Zhou2024}{{46}{2024}{{Zhou et~al.}}{{Zhou, Xu, Zheng, Chen, Li, Bu, Wu, Wang, Zhu, and Ester}}}
\gdef \@abspage@last{88}
