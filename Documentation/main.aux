\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Abstract}{1}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Contents}{1}{chapter*.1}\protected@file@percent }
\citation{Miklautz2021}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{6}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Tables}{8}{chapter*.3}\protected@file@percent }
\citation{LeCun2015}
\citation{Ren2024}
\citation{Guidotti2018,Balachandran2009}
\citation{Saisubramanian2019}
\citation{Ozyegen2022}
\citation{Plant2011}
\citation{Tjoa2023}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{9}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Background}{9}{section.1.1}\protected@file@percent }
\citation{Ruder2017,Crawshaw2020}
\citation{Bengio2013}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Problem Statement}{10}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Formal Problem Definition}{10}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Key Challenges}{10}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Research Objectives}{11}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Contributions}{11}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Methodological Contributions}{11}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Empirical Contributions}{12}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Practical Contributions}{12}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Thesis Structure}{12}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Scope and Delimitations}{13}{section.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}In Scope}{13}{subsection.1.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Out of Scope}{13}{subsection.1.6.2}\protected@file@percent }
\citation{Miklautz2021}
\citation{Min2018,Zhou2024}
\citation{Vega-Pons2011}
\citation{Strehl2002,Fred2005}
\citation{Miklautz2021}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{15}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Deep Embedded Clustering with Consensus Representations (DECCS)}{15}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Related Work in Consensus Clustering and Deep Clustering}{15}{subsection.2.1.1}\protected@file@percent }
\citation{Xie2016}
\citation{Guo2017}
\citation{Jiang2017}
\citation{Chang2017}
\citation{Yang2016}
\citation{Caron2018}
\citation{Mautz2020}
\citation{Kingma2014}
\citation{Miklautz2021}
\citation{Miklautz2021}
\citation{Vinh2010}
\citation{Zhang2021}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Visualisation of one round of DECCS\citep  {Miklautz2021}. (1) The encoder is used to embed data points $X$. (2) Clustering results are generated by applying ensemble members $\mathcal  {E} = \{KM, \ldots  , SC\}$ to $Z$. (3) Classifiers $g_i$ are trained to predict the corresponding cluster labels $\pi _i$ from $Z$. (4) $Z$ is updated via minimizing $\mathcal  {L}$.}}{17}{figure.2.1}\protected@file@percent }
\newlabel{fig:pipeline}{{2.1}{17}{Visualisation of one round of DECCS\citep {Miklautz2021}. (1) The encoder is used to embed data points $X$. (2) Clustering results are generated by applying ensemble members $\mathcal {E} = \{KM, \ldots , SC\}$ to $Z$. (3) Classifiers $g_i$ are trained to predict the corresponding cluster labels $\pi _i$ from $Z$. (4) $Z$ is updated via minimizing $\mathcal {L}$}{figure.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Methodology of DECCS}{17}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Experimental Evaluation}{17}{subsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The framework of deep descriptive clustering (DDC). DDC consists of one clustering objective, one sub-symbolic explanation objective, and one self-generated objective to maximize the consistency between clustering and explanation modules.}}{18}{figure.2.2}\protected@file@percent }
\newlabel{fig:ddc}{{2.2}{18}{The framework of deep descriptive clustering (DDC). DDC consists of one clustering objective, one sub-symbolic explanation objective, and one self-generated objective to maximize the consistency between clustering and explanation modules}{figure.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Deep Descriptive Clustering (DDC)}{18}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Key Aspects of Deep Descriptive Clustering (DDC)}{18}{subsection.2.2.1}\protected@file@percent }
\citation{Xie2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Experiments and Findings}{19}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Conclusion and Future Directions}{19}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Related Works}{19}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Deep Embedded Clustering: A General Approach to Clustering Arbitrary Similarity Measures by J. Xie, R. Girshick, and A. Farhadi (2016)}{19}{subsection.2.3.1}\protected@file@percent }
\citation{Kingma2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Architecture of the Deep Embedded Clustering (DEC) model. The model consists of a stacked autoencoder followed by a clustering layer. The autoencoder learns a low-dimensional representation of the data, which is then clustered using a clustering objective.}}{20}{figure.2.3}\protected@file@percent }
\newlabel{fig:dec}{{2.3}{20}{Architecture of the Deep Embedded Clustering (DEC) model. The model consists of a stacked autoencoder followed by a clustering layer. The autoencoder learns a low-dimensional representation of the data, which is then clustered using a clustering objective}{figure.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Auto-Encoding Variational Bayes by D. P. Kingma and M. Welling (2014)}{20}{subsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Architecture of the Variational Autoencoder (VAE) model. The encoder maps input data to a latent space, and the decoder reconstructs the data from the latent space. The model is trained to minimize reconstruction loss and regularization loss.}}{21}{figure.2.4}\protected@file@percent }
\newlabel{fig:vae}{{2.4}{21}{Architecture of the Variational Autoencoder (VAE) model. The encoder maps input data to a latent space, and the decoder reconstructs the data from the latent space. The model is trained to minimize reconstruction loss and regularization loss}{figure.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Explainable-by-Design Algorithms}{21}{subsection.2.3.3}\protected@file@percent }
\citation{davidson2018}
\citation{Sambaturu2020}
\citation{Ribeiro2016}
\citation{Rishinanda2021,Liu2022,Chhajer2022}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Post-processing Explanation Methods}{22}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Other Related Works}{22}{subsection.2.3.5}\protected@file@percent }
\citation{Chen2020}
\citation{He2020}
\citation{Caron2020}
\citation{Grill2020}
\citation{Li2021}
\citation{Zhong2021}
\citation{Shen2021}
\citation{Jaiswal2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Contrastive Learning and Deep Clustering}{23}{subsection.2.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Comprehensive Comparison of Clustering Approaches}{23}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Taxonomy of Deep Clustering Methods}{23}{subsection.2.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Taxonomy of deep clustering approaches}}{23}{table.2.1}\protected@file@percent }
\newlabel{tab:taxonomy}{{2.1}{23}{Taxonomy of deep clustering approaches}{table.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Detailed Method Comparison}{24}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Deep Embedded Clustering (DEC)}{24}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Variational Autoencoders for Clustering (VAE-based)}{24}{subsection.2.4.2}\protected@file@percent }
\citation{Xie2016}
\citation{Miklautz2021}
\citation{Zhang2021}
\@writefile{toc}{\contentsline {subsubsection}{Deep Clustering with Consensus Representations (DECCS)}{25}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Deep Descriptive Clustering (DDC)}{25}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Quantitative Performance Comparison}{26}{subsection.2.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Performance comparison of deep clustering methods on benchmark datasets}}{26}{table.2.2}\protected@file@percent }
\newlabel{tab:methods_comparison}{{2.2}{26}{Performance comparison of deep clustering methods on benchmark datasets}{table.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Qualitative Feature Comparison}{26}{subsection.2.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Qualitative comparison of clustering method features}}{26}{table.2.3}\protected@file@percent }
\newlabel{tab:qualitative_comparison}{{2.3}{26}{Qualitative comparison of clustering method features}{table.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Computational Complexity Analysis}{26}{subsection.2.4.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Computational complexity comparison}}{26}{table.2.4}\protected@file@percent }
\newlabel{tab:complexity}{{2.4}{26}{Computational complexity comparison}{table.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.6}Applicability Analysis}{27}{subsection.2.4.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces Method applicability to different data types and scenarios}}{27}{table.2.5}\protected@file@percent }
\newlabel{tab:applicability}{{2.5}{27}{Method applicability to different data types and scenarios}{table.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.7}Key Innovations by Method}{27}{subsection.2.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.8}Limitations Comparison}{28}{subsection.2.4.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.6}{\ignorespaces Method limitations and failure modes}}{28}{table.2.6}\protected@file@percent }
\newlabel{tab:limitations}{{2.6}{28}{Method limitations and failure modes}{table.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Evolution of Deep Clustering Paradigms}{28}{section.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Evolution and relationships between deep clustering methods}}{28}{figure.2.5}\protected@file@percent }
\newlabel{fig:evolution}{{2.5}{28}{Evolution and relationships between deep clustering methods}{figure.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Research Gaps Addressed}{29}{subsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Positioning in the Literature}{29}{subsection.2.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Our work at the intersection of three research areas}}{30}{figure.2.6}\protected@file@percent }
\newlabel{fig:venn}{{2.6}{30}{Our work at the intersection of three research areas}{figure.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Summary and Implications for This Research}{30}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Key Insights from the Literature}{30}{subsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Research Gap}{31}{subsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Positioning of Our Work}{31}{subsection.2.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{32}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Overview}{32}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Methodological Contributions}{32}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Research Design}{32}{subsection.3.1.2}\protected@file@percent }
\citation{Xian2019,Lampert2014}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Overview of our integrated methodology combining semantic supervision, representation learning, and consensus clustering}}{33}{figure.3.1}\protected@file@percent }
\newlabel{fig:methodology_overview}{{3.1}{33}{Overview of our integrated methodology combining semantic supervision, representation learning, and consensus clustering}{figure.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Dataset Preparation}{33}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Animals with Attributes 2 (AwA2) Dataset}{33}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Semantic Attribute Structure}{33}{subsection.3.2.2}\protected@file@percent }
\citation{Bengio2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Data Preprocessing Pipeline}{35}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Image Preprocessing}{35}{subsection.3.2.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Image Preprocessing Pipeline}}{35}{algorithm.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Attribute Normalization}{35}{ALC@unique.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dataset Splitting}{35}{equation.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Data Loading Implementation}{35}{subsection.3.2.4}\protected@file@percent }
\citation{Akata2015,Akata2016}
\citation{Chen2021}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Model Architecture}{36}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Baseline Autoencoder (AE)}{36}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Encoder Architecture}{36}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Decoder Architecture}{36}{equation.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Baseline Loss Function}{36}{equation.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Constrained Autoencoder (CAE)}{36}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architecture Extension}{36}{subsection.3.3.2}\protected@file@percent }
\citation{Yang2017}
\@writefile{toc}{\contentsline {subsubsection}{Multi-Objective Loss Function}{37}{equation.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Hyperparameter Selection}{37}{equation.3.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Consensus Clustering Integration (DECCS)}{37}{subsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Consensus Representation Learning}{37}{subsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Ensemble Clustering Algorithms}{38}{equation.3.11}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Ensemble clustering algorithms and their characteristics}}{38}{table.3.1}\protected@file@percent }
\newlabel{tab:ensemble_algorithms}{{3.1}{38}{Ensemble clustering algorithms and their characteristics}{table.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Consensus Matrix Construction}{38}{table.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Consensus Consistency Loss}{38}{equation.3.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Full DECCS Loss}{38}{equation.3.15}\protected@file@percent }
\citation{Kingma2015}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Training Procedure}{39}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Optimization Strategy}{39}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Optimizer Configuration}{39}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Batch Processing}{39}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Mixed Precision Training}{39}{subsection.3.4.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Mixed Precision Training Step}}{39}{algorithm.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Training Algorithms}{40}{subsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Baseline Autoencoder Training}{40}{subsection.3.4.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Train Baseline Autoencoder}}{40}{algorithm.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Constrained Autoencoder Training}{40}{ALC@unique.20}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Train Constrained Autoencoder}}{40}{algorithm.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{DECCS Training with Consensus}{41}{ALC@unique.32}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Train DECCS with Consensus}}{41}{algorithm.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Clustering and Evaluation}{41}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Embedding Extraction}{41}{subsection.3.5.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Extract Embeddings}}{41}{algorithm.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Consensus Clustering}{41}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Ensemble Execution}{41}{subsection.3.5.2}\protected@file@percent }
\citation{Vinh2010}
\citation{Hubert1985}
\citation{Kuhn1955}
\citation{Rousseeuw1987}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Consensus Clustering Evaluation}}{42}{algorithm.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Evaluation Metrics}{42}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Clustering Performance Metrics}{42}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Normalized Mutual Information (NMI)}{42}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Adjusted Rand Index (ARI)}{42}{equation.3.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Clustering Accuracy (ACC)}{42}{equation.3.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Silhouette Score}{42}{equation.3.19}\protected@file@percent }
\citation{Paszke2019}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Cluster Explanation Generation}{43}{section.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Attribute-Based Characterization}{43}{subsection.3.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Top-K Attribute Selection}{43}{subsection.3.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Explanation Quality Metrics}{43}{subsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Cluster Purity}{43}{subsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Attribute Discriminativeness}{43}{equation.3.23}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Implementation Details}{43}{section.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Software Framework}{43}{subsection.3.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Hardware Configuration}{43}{subsection.3.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Reproducibility}{44}{subsection.3.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Limitations and Design Choices}{44}{section.3.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Methodological Limitations}{44}{subsection.3.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}Design Rationale}{44}{subsection.3.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Summary of Methodological Choices}{45}{section.3.9}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Complete configuration summary}}{45}{table.3.2}\protected@file@percent }
\newlabel{tab:table}{{3.2}{45}{Complete configuration summary}{table.3.2}{}}
\citation{Xian2019}
\citation{Kingma2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{46}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experimental Setup}{46}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Hyperparameters}{46}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Hyperparameter Tuning on Sample Subset}{46}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Grid Search Configuration}{47}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Sample-Based Tuning Results}{47}{subsection.4.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Hyperparameter tuning results on sample subset (N=160). \textbf  {Caution:} These results may not generalize to the full dataset.}}{47}{table.4.1}\protected@file@percent }
\newlabel{tab:tuning_results}{{4.1}{47}{Hyperparameter tuning results on sample subset (N=160). \textbf {Caution:} These results may not generalize to the full dataset}{table.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Full-Scale Results}{48}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Current State}{48}{subsection.4.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Sample vs Full-Scale Performance Comparison}}{48}{table.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Comparison: Sample vs. Full Dataset}{48}{subsection.4.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Comparison of sample-based tuning vs. earlier full-dataset runs}}{48}{table.4.3}\protected@file@percent }
\newlabel{tab:sample_vs_full}{{4.3}{48}{Comparison of sample-based tuning vs. earlier full-dataset runs}{table.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Training Dynamics}{48}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Loss Curves}{48}{subsection.4.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Training loss for baseline Autoencoder (AE). The reconstruction loss decreases smoothly from approximately 0.032 to 0.006 over 30 epochs, indicating successful convergence of the reconstruction objective.}}{49}{figure.4.1}\protected@file@percent }
\newlabel{fig:training_loss_ae}{{4.1}{49}{Training loss for baseline Autoencoder (AE). The reconstruction loss decreases smoothly from approximately 0.032 to 0.006 over 30 epochs, indicating successful convergence of the reconstruction objective}{figure.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Training loss for Constrained Autoencoder (CAE). Multiple loss components are shown: total loss (blue), reconstruction loss, and tag prediction loss. All components show decreasing trends over 30 epochs.}}{50}{figure.4.2}\protected@file@percent }
\newlabel{fig:training_loss_cae}{{4.2}{50}{Training loss for Constrained Autoencoder (CAE). Multiple loss components are shown: total loss (blue), reconstruction loss, and tag prediction loss. All components show decreasing trends over 30 epochs}{figure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Training loss for DECCS mode. \textbf  {Notable pattern:} Periodic spikes occur every 5 epochs, corresponding to when the consensus matrix is rebuilt. The spikes indicate that the new consensus targets temporarily increase the loss before the model adapts.}}{51}{figure.4.3}\protected@file@percent }
\newlabel{fig:training_loss_deccs}{{4.3}{51}{Training loss for DECCS mode. \textbf {Notable pattern:} Periodic spikes occur every 5 epochs, corresponding to when the consensus matrix is rebuilt. The spikes indicate that the new consensus targets temporarily increase the loss before the model adapts}{figure.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Embedding Space Visualization}{51}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}PCA Projections}{51}{subsection.4.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces PCA projection of baseline AE embeddings. Points are colored by cluster assignment. The embedding space shows some structure with outliers on the right side.}}{52}{figure.4.4}\protected@file@percent }
\newlabel{fig:pca_ae}{{4.4}{52}{PCA projection of baseline AE embeddings. Points are colored by cluster assignment. The embedding space shows some structure with outliers on the right side}{figure.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces PCA projection of CAE embeddings. The distribution shows a similar pattern to AE with a concentrated core and extended outliers.}}{52}{figure.4.5}\protected@file@percent }
\newlabel{fig:pca_cae}{{4.5}{52}{PCA projection of CAE embeddings. The distribution shows a similar pattern to AE with a concentrated core and extended outliers}{figure.4.5}{}}
\citation{vanderMaaten2014}
\citation{McInnes2018}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces PCA projection of DECCS embeddings. The embedding space shows a roughly arc-shaped distribution, with colors representing cluster assignments that do not show clear separation.}}{53}{figure.4.6}\protected@file@percent }
\newlabel{fig:pca_deccs}{{4.6}{53}{PCA projection of DECCS embeddings. The embedding space shows a roughly arc-shaped distribution, with colors representing cluster assignments that do not show clear separation}{figure.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}t-SNE Visualization}{53}{subsection.4.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces t-SNE projection of DECCS embeddings colored by cluster assignment. Local structure is visible with groups of same-colored points forming coherent regions, particularly in peripheral areas.}}{54}{figure.4.7}\protected@file@percent }
\newlabel{fig:tsne}{{4.7}{54}{t-SNE projection of DECCS embeddings colored by cluster assignment. Local structure is visible with groups of same-colored points forming coherent regions, particularly in peripheral areas}{figure.4.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Identified Challenges}{54}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Training Dynamics in DECCS Mode}{54}{subsection.4.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}ARI Near Zero}{55}{subsection.4.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}Sensitivity to Hyperparameters}{55}{subsection.4.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Summary of Results}{55}{section.4.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Summary of implementation status and results}}{55}{table.4.4}\protected@file@percent }
\newlabel{tab:status_summary}{{4.4}{55}{Summary of implementation status and results}{table.4.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Discussion}{57}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Overview}{57}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Interpretation of Results}{57}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Analysis of Scaling Failure}{57}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Consensus Matrix Scalability}{57}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Multi-Objective Optimization Challenges}{57}{subsection.5.2.1}\protected@file@percent }
\citation{Ruder2017,Crawshaw2020}
\citation{Ren2024,Zhou2024}
\@writefile{toc}{\contentsline {subsubsection}{Sample Representativeness}{58}{equation.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Comparison with DDC}{58}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Missing Pairwise Consistency Loss}{58}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Single Model vs. Ensemble Complexity}{58}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architecture Differences}{59}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Potential Issues to Investigate}{59}{subsection.5.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Metric Calculation}{59}{subsection.5.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training Dynamics}{59}{subsection.5.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Comparison with Research Hypotheses}{59}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Hypothesis 1: DDC Integration Improves Interpretability}{59}{subsection.5.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Hypothesis 2: Interpretability Does Not Compromise Performance}{59}{subsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Hypothesis 3: Consensus Clustering Enhances Robustness}{60}{subsection.5.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Limitations and Challenges}{60}{section.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Implementation Limitations}{60}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Incomplete DECCS Integration}{60}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Small-Scale Experiments}{60}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Simple Architecture}{60}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Methodological Limitations}{61}{subsection.5.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dependence on Predefined Attributes}{61}{subsection.5.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Class-Level vs. Instance-Level Attributes}{61}{subsection.5.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Binary Clustering Evaluation}{61}{subsection.5.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Dataset-Specific Challenges}{61}{subsection.5.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Class Imbalance}{61}{subsection.5.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Attribute Noise}{61}{subsection.5.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Theoretical Implications}{61}{section.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Multi-Task Learning for Clustering}{61}{subsection.5.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}The Value of Human-Aligned Representations}{62}{subsection.5.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Consensus as Uncertainty Quantification}{62}{subsection.5.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Practical Implications}{62}{section.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}When to Use This Approach}{62}{subsection.5.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}When Other Methods May Be Preferable}{62}{subsection.5.6.2}\protected@file@percent }
\citation{Miklautz2021}
\citation{Zhang2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}Deployment Considerations}{63}{subsection.5.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Comparison with Related Work}{63}{section.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}Comparison with Original DECCS}{63}{subsection.5.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}Comparison with Original DDC}{63}{subsection.5.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Summary}{64}{section.5.8}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{65}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Summary of Work}{65}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}What Was Accomplished}{65}{subsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}What Did Not Work}{65}{subsection.6.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Summary of Results}}{65}{table.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Research Questions Answered}{65}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}RQ1: How can DDC be integrated into DECCS?}{66}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}RQ2: Impact on Clustering Performance}{66}{subsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}RQ3: Performance on AwA2 Dataset}{66}{subsection.6.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Lessons Learned}{66}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Full-Scale Validation is Essential}{66}{subsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Multi-Objective Optimization is Difficult}{66}{subsection.6.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Negative Results Have Value}{67}{subsection.6.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Limitations}{67}{section.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Future Work}{67}{section.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Immediate Priorities}{67}{subsection.6.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Longer-Term Research}{67}{subsection.6.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Final Remarks}{68}{section.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix A}{69}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Appendix A: Hyperparameter Optimization}{70}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Grid Search Configuration}{70}{section.B.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Complete Results Table}{70}{section.B.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Complete hyperparameter grid search results for DECCS mode}}{70}{table.B.1}\protected@file@percent }
\newlabel{tab:hyperparam_full}{{B.1}{70}{Complete hyperparameter grid search results for DECCS mode}{table.B.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Analysis of Hyperparameter Effects}{70}{section.B.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.1}Impact of Consensus Weight ($\lambda _{consensus}$)}{70}{subsection.B.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Effect of consensus weight on clustering performance (with $\lambda _{tag}=1.0$)}}{71}{figure.B.1}\protected@file@percent }
\newlabel{fig:lambda_consensus_effect}{{B.1}{71}{Effect of consensus weight on clustering performance (with $\lambda _{tag}=1.0$)}{figure.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.2}Impact of Tag Supervision Weight ($\lambda _{tag}$)}{71}{subsection.B.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Effect of tag supervision weight on clustering performance (with $\lambda _{consensus}=0.2$)}}{71}{figure.B.2}\protected@file@percent }
\newlabel{fig:lambda_tag_effect}{{B.2}{71}{Effect of tag supervision weight on clustering performance (with $\lambda _{consensus}=0.2$)}{figure.B.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.4}Best Configuration}{72}{section.B.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B.5}Convergence Analysis}{72}{section.B.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces Training convergence with optimal hyperparameters}}{72}{figure.B.3}\protected@file@percent }
\newlabel{fig:convergence_best}{{B.3}{72}{Training convergence with optimal hyperparameters}{figure.B.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.6}Sensitivity Analysis}{73}{section.B.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.6.1}Robustness to Initialization}{73}{subsection.B.6.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Performance variance across random seeds}}{73}{table.B.2}\protected@file@percent }
\newlabel{tab:seed_variance}{{B.2}{73}{Performance variance across random seeds}{table.B.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.7}Computational Cost Analysis}{73}{section.B.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.3}{\ignorespaces Training time per epoch for different configurations}}{73}{table.B.3}\protected@file@percent }
\newlabel{tab:timing}{{B.3}{73}{Training time per epoch for different configurations}{table.B.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Appendix B}{74}{appendix.C}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {D}Appendix B: Additional Visualizations and Cluster Analysis}{75}{appendix.D}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {D.1}Detailed Cluster Descriptions}{75}{section.D.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1.1}Complete Cluster Attribute Analysis}{75}{subsection.D.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {D.1}{\ignorespaces Representative cluster characterizations with top-5 attributes}}{75}{table.D.1}\protected@file@percent }
\newlabel{tab:all_clusters}{{D.1}{75}{Representative cluster characterizations with top-5 attributes}{table.D.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1.2}Cluster Purity Analysis}{75}{subsection.D.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D.2}Embedding Space Analysis}{76}{section.D.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2.1}Comparison of Embedding Visualizations Across Methods}{76}{subsection.D.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {D.1}{\ignorespaces PCA projection of baseline autoencoder (AE) embeddings. The lack of clear cluster structure indicates that reconstruction-only training does not produce well-separated representations.}}{76}{figure.D.1}\protected@file@percent }
\newlabel{fig:pca_ae}{{D.1}{76}{PCA projection of baseline autoencoder (AE) embeddings. The lack of clear cluster structure indicates that reconstruction-only training does not produce well-separated representations}{figure.D.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D.2}{\ignorespaces PCA projection of Constrained Autoencoder (CAE) embeddings. Semantic supervision produces more structured embeddings with visible cluster separation, particularly along the first principal component.}}{77}{figure.D.2}\protected@file@percent }
\newlabel{fig:pca_cae}{{D.2}{77}{PCA projection of Constrained Autoencoder (CAE) embeddings. Semantic supervision produces more structured embeddings with visible cluster separation, particularly along the first principal component}{figure.D.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D.3}{\ignorespaces PCA projection of DECCS embeddings with consensus clustering. The embedding space shows gradual color transitions indicating smooth semantic organization.}}{78}{figure.D.3}\protected@file@percent }
\newlabel{fig:pca_deccs}{{D.3}{78}{PCA projection of DECCS embeddings with consensus clustering. The embedding space shows gradual color transitions indicating smooth semantic organization}{figure.D.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2.2}t-SNE Visualization}{78}{subsection.D.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {D.4}{\ignorespaces t-SNE projection of DECCS embeddings colored by cluster assignment. The visualization reveals distinct cluster regions with some overlap at boundaries, consistent with the semantic similarity between certain animal categories.}}{79}{figure.D.4}\protected@file@percent }
\newlabel{fig:tsne_full}{{D.4}{79}{t-SNE projection of DECCS embeddings colored by cluster assignment. The visualization reveals distinct cluster regions with some overlap at boundaries, consistent with the semantic similarity between certain animal categories}{figure.D.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2.3}Embedding Dimension Analysis}{79}{subsection.D.2.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {D.2}{\ignorespaces Variance explained by principal components}}{79}{table.D.2}\protected@file@percent }
\newlabel{tab:pca_variance}{{D.2}{79}{Variance explained by principal components}{table.D.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D.3}Training Dynamics}{80}{section.D.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3.1}Loss Curves Comparison}{80}{subsection.D.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {D.5}{\ignorespaces Training loss for baseline autoencoder (AE). Smooth convergence with reconstruction loss only.}}{80}{figure.D.5}\protected@file@percent }
\newlabel{fig:loss_ae}{{D.5}{80}{Training loss for baseline autoencoder (AE). Smooth convergence with reconstruction loss only}{figure.D.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D.6}{\ignorespaces Training loss for Constrained Autoencoder (CAE). Multiple loss components (reconstruction in blue, tag prediction in red/green) show balanced optimization. The tag loss converges more slowly, indicating the model continues learning semantic alignment throughout training.}}{81}{figure.D.6}\protected@file@percent }
\newlabel{fig:loss_cae}{{D.6}{81}{Training loss for Constrained Autoencoder (CAE). Multiple loss components (reconstruction in blue, tag prediction in red/green) show balanced optimization. The tag loss converges more slowly, indicating the model continues learning semantic alignment throughout training}{figure.D.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D.7}{\ignorespaces Training loss for DECCS mode. Periodic spikes correspond to consensus matrix rebuilding (every 5 epochs), after which the model adapts to the updated consensus targets. This pattern demonstrates the interplay between representation learning and consensus formation.}}{82}{figure.D.7}\protected@file@percent }
\newlabel{fig:loss_deccs}{{D.7}{82}{Training loss for DECCS mode. Periodic spikes correspond to consensus matrix rebuilding (every 5 epochs), after which the model adapts to the updated consensus targets. This pattern demonstrates the interplay between representation learning and consensus formation}{figure.D.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D.4}Error Analysis}{82}{section.D.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.4.1}Common Misclassification Patterns}{82}{subsection.D.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {D.3}{\ignorespaces Common misclassification patterns}}{83}{table.D.3}\protected@file@percent }
\newlabel{tab:errors}{{D.3}{83}{Common misclassification patterns}{table.D.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D.5}Attribute Importance Analysis}{83}{section.D.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.5.1}Most Discriminative Attributes}{83}{subsection.D.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {D.4}{\ignorespaces Top 15 most discriminative attributes}}{83}{table.D.4}\protected@file@percent }
\newlabel{tab:attr_importance}{{D.4}{83}{Top 15 most discriminative attributes}{table.D.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D.6}Cluster Stability Analysis}{84}{section.D.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.6.1}Bootstrap Stability}{84}{subsection.D.6.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {D.5}{\ignorespaces Cluster stability scores (Adjusted Rand Index between bootstrap samples)}}{84}{table.D.5}\protected@file@percent }
\newlabel{tab:stability}{{D.5}{84}{Cluster stability scores (Adjusted Rand Index between bootstrap samples)}{table.D.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D.7}Implementation Details}{85}{section.D.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.7.1}Model Architecture Specifications}{85}{subsection.D.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.7.2}Training Configuration}{86}{subsection.D.7.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {D.6}{\ignorespaces Complete training configuration}}{86}{table.D.6}\protected@file@percent }
\newlabel{tab:training_config}{{D.6}{86}{Complete training configuration}{table.D.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D.8}Dataset Statistics}{86}{section.D.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.8.1}AwA2 Dataset Breakdown}{86}{subsection.D.8.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {D.7}{\ignorespaces AwA2 dataset statistics}}{86}{table.D.7}\protected@file@percent }
\newlabel{tab:dataset_stats}{{D.7}{86}{AwA2 dataset statistics}{table.D.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.8.2}Attribute Distribution}{86}{subsection.D.8.2}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{bibliography}
\@writefile{lof}{\contentsline {figure}{\numberline {D.8}{\ignorespaces Mean attribute values by category across all classes}}{87}{figure.D.8}\protected@file@percent }
\newlabel{fig:attr_dist}{{D.8}{87}{Mean attribute values by category across all classes}{figure.D.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D.9}Ensemble Clustering Details}{87}{section.D.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.9.1}Base Clustering Algorithms}{87}{subsection.D.9.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {D.8}{\ignorespaces Ensemble clustering algorithms and configurations}}{87}{table.D.8}\protected@file@percent }
\newlabel{tab:ensemble_config}{{D.8}{87}{Ensemble clustering algorithms and configurations}{table.D.8}{}}
\bibcite{Akata2015}{{1}{2015}{{Akata et~al.}}{{Akata, Reed, Walter, Lee, and Schiele}}}
\bibcite{Akata2016}{{2}{2016}{{Akata et~al.}}{{Akata, Perronnin, Harchaoui, and Schmid}}}
\bibcite{Balachandran2009}{{3}{2009}{{Balachandran et~al.}}{{Balachandran, Deepak, and Khemani}}}
\bibcite{Bengio2013}{{4}{2013}{{Bengio et~al.}}{{Bengio, Courville, and Vincent}}}
\bibcite{Caron2018}{{5}{2018}{{Caron et~al.}}{{Caron, Bojanowski, Joulin, and Douze}}}
\bibcite{Caron2020}{{6}{2020}{{Caron et~al.}}{{Caron, Misra, Mairal, Goyal, Bojanowski, and Joulin}}}
\bibcite{Chang2017}{{7}{2017}{{Chang et~al.}}{{Chang, Wang, Meng, Xiang, and Pan}}}
\bibcite{Chen2020}{{8}{2020}{{Chen et~al.}}{{Chen, Kornblith, Norouzi, and Hinton}}}
\bibcite{Chen2021}{{9}{2021}{{Chen and He}}{{}}}
\bibcite{Chhajer2022}{{10}{2022}{{Chhajer and Moniri}}{{}}}
\bibcite{Fred2005}{{11}{2005}{{Fred and Jain}}{{}}}
\bibcite{Grill2020}{{12}{2020}{{Grill et~al.}}{{Grill, Strub, Altch\'{e}, Tallec, Richemond, Buchatskaya, Doersch, Pires, Guo, Azar, Piot, Kavukcuoglu, Munos, and Valko}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{88}{appendix*.4}\protected@file@percent }
\bibcite{Guidotti2018}{{13}{2018}{{Guidotti et~al.}}{{Guidotti, Monreale, Ruggieri, Turini, Giannotti, and Pedreschi}}}
\bibcite{Guo2017}{{14}{2017}{{Guo et~al.}}{{Guo, Gao, Liu, and Yin}}}
\bibcite{He2020}{{15}{2020}{{He et~al.}}{{He, Fan, Wu, Xie, and Girshick}}}
\bibcite{Hubert1985}{{16}{1985}{{Hubert and Arabie}}{{}}}
\bibcite{Jaiswal2021}{{17}{2021}{{Jaiswal et~al.}}{{Jaiswal, Babu, Zadeh, Banerjee, and Makedon}}}
\bibcite{Jiang2017}{{18}{2017}{{Jiang et~al.}}{{Jiang, Zheng, Tan, Tang, and Zhou}}}
\bibcite{Kuhn1955}{{19}{1955}{{Kuhn}}{{}}}
\bibcite{Lampert2014}{{20}{2014}{{Lampert et~al.}}{{Lampert, Nickisch, and Harmeling}}}
\bibcite{LeCun2015}{{21}{2015}{{LeCun et~al.}}{{LeCun, Bengio, and Hinton}}}
\bibcite{Li2021}{{22}{2021}{{Li et~al.}}{{Li, Hu, Liu, Peng, Zhou, and Peng}}}
\bibcite{Liu2022}{{23}{2022}{{Liu et~al.}}{{Liu, Wang, Liu, and Yu}}}
\bibcite{Mautz2020}{{24}{2020}{{Mautz et~al.}}{{Mautz, Plant, and Bhm}}}
\bibcite{McInnes2018}{{25}{2018}{{McInnes et~al.}}{{McInnes, Healy, and Melville}}}
\bibcite{Min2018}{{26}{2018}{{Min et~al.}}{{Min, Guo, Liu, Zhang, Cui, and Long}}}
\bibcite{Ozyegen2022}{{27}{2022}{{Ozyegen et~al.}}{{Ozyegen, Prayogo, Cevik, and Basar}}}
\bibcite{Paszke2019}{{28}{2019}{{Paszke et~al.}}{{Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kpf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala}}}
\bibcite{Plant2011}{{29}{2011}{{Plant and Bhm}}{{}}}
\bibcite{Ren2024}{{30}{2024}{{Ren et~al.}}{{Ren, Pu, Yang, Xu, Li, Pu, Yu, and He}}}
\bibcite{Ribeiro2016}{{31}{2016}{{Ribeiro et~al.}}{{Ribeiro, Singh, and Guestrin}}}
\bibcite{Rishinanda2021}{{32}{2021}{{Rishinanda and Sebag}}{{}}}
\bibcite{Rousseeuw1987}{{33}{1987}{{Rousseeuw}}{{}}}
\bibcite{Saisubramanian2019}{{34}{2019}{{Saisubramanian et~al.}}{{Saisubramanian, Galhotra, and Zilberstein}}}
\bibcite{Sambaturu2020}{{35}{2020}{{Sambaturu et~al.}}{{Sambaturu, Gupta, Davidson, Ravi, Vullikanti, and Warren}}}
\bibcite{Shen2021}{{36}{2021}{{Shen et~al.}}{{Shen, Shen, Wang, Qin, Torr, and Shao}}}
\bibcite{Strehl2002}{{37}{2002}{{Strehl and Ghosh}}{{}}}
\bibcite{Tjoa2023}{{38}{2021}{{Tjoa and Guan}}{{}}}
\bibcite{vanderMaaten2014}{{39}{2014}{{van~der Maaten}}{{}}}
\bibcite{Vega-Pons2011}{{40}{2011}{{Vega-Pons and Ruiz-Shulcloper}}{{}}}
\bibcite{Vinh2010}{{41}{2010}{{Vinh et~al.}}{{Vinh, Epps, and Bailey}}}
\bibcite{Xian2019}{{42}{2019}{{Xian et~al.}}{{Xian, Lampert, Schiele, and Akata}}}
\bibcite{Yang2017}{{43}{2017}{{Yang et~al.}}{{Yang, Fu, Sidiropoulos, and Hong}}}
\bibcite{Yang2016}{{44}{2016}{{Yang et~al.}}{{Yang, Parikh, and Batra}}}
\bibcite{Zhang2021}{{45}{2021}{{Zhang and Davidson}}{{}}}
\bibcite{Zhong2021}{{46}{2021}{{Zhong et~al.}}{{Zhong, Wu, Chen, Huang, Deng, Nie, Lin, and Hua}}}
\bibcite{Zhou2024}{{47}{2024}{{Zhou et~al.}}{{Zhou, Xu, Zheng, Chen, Li, Bu, Wu, Wang, Zhu, and Ester}}}
\gdef \@abspage@last{93}
