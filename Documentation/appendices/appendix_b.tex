\section{Embedding Visualizations}

This appendix presents the embedding space visualizations generated from experiments.

\subsection{PCA Projections}

Figures~\ref{fig:pca_ae_app}--\ref{fig:pca_deccs_app} show PCA projections of learned embeddings from each experimental mode.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/results_ae_pca.png}
    \caption{PCA projection of baseline Autoencoder (AE) embeddings. Colors represent cluster assignments from K-Means. The distribution shows a concentrated core with outlier points extending to the right.}
    \label{fig:pca_ae_app}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/results_cae_pca.png}
    \caption{PCA projection of Constrained Autoencoder (CAE) embeddings. Similar structure to AE with a dense core and extended outliers, suggesting tag supervision did not fundamentally change the embedding geometry.}
    \label{fig:pca_cae_app}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/results_deccs_pca.png}
    \caption{PCA projection of DECCS embeddings. The distribution forms a roughly arc-shaped pattern. Color gradients do not correspond to spatially distinct clusters.}
    \label{fig:pca_deccs_app}
\end{figure}

\textbf{Observations:}
\begin{itemize}
    \item The DECCS embeddings show more structured distribution with the consensus loss
    \item Outlier points in AE/CAE may represent challenging samples
    \item DECCS embeddings form an arc shape, indicating the consensus loss shapes geometry
    \item Color gradients show some correspondence with spatial regions, consistent with NMI=0.642
\end{itemize}

\subsection{t-SNE Visualization}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/results_tsne.png}
    \caption{t-SNE projection of DECCS embeddings. Multiple distinct clusters are visible (see colorbar), with local structure showing groups of same-colored points forming coherent regions.}
    \label{fig:tsne_app}
\end{figure}

\textbf{t-SNE Interpretation Notes:}
\begin{itemize}
    \item t-SNE emphasizes local structure over global structure
    \item Coherent same-colored clusters are visible, particularly in peripheral regions
    \item Some central mixing remains, indicating room for improvement
    \item The visualization is consistent with the achieved NMI=0.642
\end{itemize}

\section{Training Loss Curves}

\subsection{Baseline Autoencoder}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/results_ae_loss.png}
    \caption{Training loss for baseline Autoencoder over 30 epochs. Reconstruction loss (MSE) decreases smoothly from approximately 0.032 to 0.006.}
    \label{fig:loss_ae_app}
\end{figure}

\textbf{Analysis:}
\begin{itemize}
    \item Smooth, monotonic decrease indicates stable optimization
    \item Loss plateau after epoch 20 suggests convergence
    \item Final loss of $\sim$0.006 indicates reasonable reconstruction quality
\end{itemize}

\subsection{Constrained Autoencoder}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/results_cae_loss.png}
    \caption{Training loss for Constrained Autoencoder. Multiple curves show total loss, reconstruction loss, and tag prediction loss components.}
    \label{fig:loss_cae_app}
\end{figure}

\textbf{Analysis:}
\begin{itemize}
    \item All loss components decrease, indicating multi-task optimization is working
    \item Total loss decreases from $\sim$0.29 to $\sim$0.22
    \item No oscillation or instability visible
    \item Tag loss (green/red) converges more slowly than reconstruction loss
\end{itemize}

\subsection{DECCS Mode}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/results_deccs_loss.png}
    \caption{Training loss for DECCS mode showing characteristic periodic spikes every 5 epochs corresponding to consensus matrix rebuilding.}
    \label{fig:loss_deccs_app}
\end{figure}

\textbf{Critical Analysis:}
\begin{itemize}
    \item Periodic spikes occur exactly every 5 epochs (at epochs 5, 10, 15, 20, 25, 30)
    \item Spikes correspond to consensus matrix rebuilding in the code
    \item Spike magnitude does NOT decrease over time---this is problematic
    \item Between spikes, loss decreases normally
    \item The spikes indicate the new consensus targets are inconsistent with the model's current state
\end{itemize}

\textbf{This pattern is the primary diagnostic indicator of the training instability discussed in Chapter 5.}

\section{AwA2 Dataset Statistics}

\subsection{Dataset Overview}

\begin{table}[H]
    \centering
    \caption{AwA2 dataset statistics}
    \begin{tabular}{lc}
        \hline
        \textbf{Statistic} & \textbf{Value} \\
        \hline
        Total Images & 37,322 \\
        Number of Classes & 50 \\
        Attributes per Class & 85 \\
        Training Classes (standard split) & 40 \\
        Test Classes (standard split) & 10 \\
        Train/Test Split Used (this work) & 80/20 random \\
        \hline
    \end{tabular}
\end{table}

\textbf{Note:} We use a random 80/20 split rather than the standard zero-shot learning split (40 train classes, 10 test classes). This means our training includes images from all 50 classes.

\subsection{Attribute Categories}

The 85 attributes fall into several semantic categories:

\begin{table}[H]
    \centering
    \caption{Attribute categories in AwA2}
    \begin{tabular}{lp{8cm}}
        \hline
        \textbf{Category} & \textbf{Example Attributes} \\
        \hline
        Color & black, white, blue, brown, gray, orange, red, yellow \\
        Texture & patches, spots, stripes, furry, hairless, toughskin \\
        Size & big, small, bulbous, lean \\
        Body Parts & flippers, hands, hooves, paws, tail, claws, tusks \\
        Locomotion & walks, swims, flys, hops, tunnels \\
        Behavior & fast, slow, strong, weak, active, nocturnal \\
        Diet & fish, meat, plankton, vegetation, insects \\
        Habitat & arctic, coastal, desert, forest, jungle, ocean \\
        Social & group, solitary, domestic \\
        \hline
    \end{tabular}
\end{table}

\subsection{Sample Classes}

\begin{table}[H]
    \centering
    \caption{Sample of AwA2 classes}
    \begin{tabular}{cl}
        \hline
        \textbf{Class ID} & \textbf{Class Name} \\
        \hline
        1 & antelope \\
        2 & grizzly+bear \\
        3 & killer+whale \\
        13 & tiger \\
        31 & giraffe \\
        38 & zebra \\
        43 & lion \\
        50 & dolphin \\
        \hline
    \end{tabular}
\end{table}

\section{Framework Diagrams}

\subsection{DECCS Framework}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figs/deccs.png}
    \caption{DECCS framework from \cite{Miklautz2021}. (1) Encoder embeds input data. (2) Ensemble clustering algorithms generate base partitions. (3) Classifiers approximate partitions. (4) Representation is updated via consensus loss.}
    \label{fig:deccs_framework}
\end{figure}

\subsection{DDC Framework}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figs/dec.png}
    \caption{Deep Descriptive Clustering (DDC) framework from \cite{Zhang2021}. Combines clustering objective with class-level explanations via Integer Linear Programming.}
    \label{fig:ddc_framework}
\end{figure}

\section{Placeholder: Results to Add After Debugging}

The following sections are placeholders for results to be added once the implementation issues are resolved:

\subsection{Cluster Descriptions}

\begin{table}[H]
    \centering
    \caption{Placeholder: Cluster attribute descriptions}
    \begin{tabular}{lp{8cm}}
        \hline
        \textbf{Cluster} & \textbf{Top-5 Attributes} \\
        \hline
        01 & \texttt{[TO BE COMPUTED AFTER FIX]} \\
        02 & \texttt{[TO BE COMPUTED AFTER FIX]} \\
        ... & ... \\
        \hline
    \end{tabular}
\end{table}

\subsection{Cluster Purity Analysis}

\begin{table}[H]
    \centering
    \caption{Placeholder: Cluster purity metrics}
    \begin{tabular}{cccc}
        \hline
        \textbf{Cluster} & \textbf{Size} & \textbf{Purity} & \textbf{Dominant Class} \\
        \hline
        \multicolumn{4}{c}{\texttt{[TO BE COMPUTED AFTER FIX]}} \\
        \hline
    \end{tabular}
\end{table}

\subsection{Confusion Matrix}

A confusion matrix between cluster assignments and ground truth labels will be computed once clustering performance improves above random baseline.

\subsection{Sample Images per Cluster}

The \texttt{save\_cluster\_examples()} function saves sample images from each cluster to the \texttt{cluster\_samples/} directory. These can be visually inspected to assess whether clusters contain semantically related images.

\textbf{Current status:} With random-level clustering, cluster samples are expected to be mixed without semantic coherence. This will be revisited after debugging.
