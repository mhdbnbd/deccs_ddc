\section{Experimental Setup}

All experiments were conducted on the Animals with Attributes 2 (AwA2) dataset \citep{Xian2019}, which contains 37,322 images across 50 animal classes, annotated with 85 semantic attributes per class. We evaluated the proposed approach under four experimental configurations:

\begin{itemize}
    \item \textbf{Baseline Autoencoder (AE)}: Reconstruction-based representation learning without semantic supervision.
    \item \textbf{Oracle}: Upper-bound performance using ground-truth attribute vectors concatenated with learned embeddings.
    \item \textbf{Constrained Autoencoder (CAE)}: Autoencoder with auxiliary attribute prediction supervision.
    \item \textbf{DECCS}: Consensus-based clustering with symbolic supervision.
\end{itemize}

\subsection{Hyperparameters}

The hyperparameter configuration was determined through grid search on a sample subset:

\begin{itemize}
    \item Consensus weight: $\lambda_{\text{consensus}} = 0.2$ (tuned)
    \item Tag supervision weight: $\lambda_{\text{tag}} = 0.5$ (tuned)
    \item Training epochs: 10 (for tuning), 4--30 (for other experiments)
    \item Batch size: 256
    \item Optimizer: Adam \citep{Kingma2015} (lr $= 0.001$)
    \item Embedding dimension: 128
    \item Image resolution: $128 \times 128$
    \item Train/test split: 80/20 random split
\end{itemize}

\section{Hyperparameter Tuning on Sample Subset}

\textbf{Important Note:} Hyperparameter tuning was performed on a \textbf{small sample subset} (160 images) for computational efficiency. These results indicate promising directions but require validation on the full dataset.

\subsection{Grid Search Configuration}

A tuning script (\texttt{tune\_hyperparams.py}) was developed to automate grid search:
\begin{itemize}
    \item $\lambda_{consensus} \in \{0.05, 0.1, 0.2\}$
    \item $\lambda_{tag} \in \{0.5, 1.0, 1.5\}$
    \item Epochs: 10
    \item Dataset: Sample subset (N=160 images, \texttt{--use\_sample} flag)
\end{itemize}

\subsection{Sample-Based Tuning Results}

Table~\ref{tab:tuning_results} presents the results from hyperparameter tuning on the sample subset.

\begin{table}[H]
    \centering
    \caption{Hyperparameter tuning results on sample subset (N=160). \textbf{Caution:} These results may not generalize to the full dataset.}
    \label{tab:tuning_results}
    \begin{tabular}{cc|cccc}
        \hline
        $\lambda_{cons}$ & $\lambda_{tag}$ & \textbf{NMI} & \textbf{ACC} & \textbf{ARI} & \textbf{Silhouette} \\
        \hline
        0.05 & 0.5 & 0.616 & 0.288 & -0.008 & 0.228 \\
        0.05 & 1.0 & 0.597 & 0.269 & -0.009 & 0.193 \\
        0.05 & 1.5 & 0.637 & 0.306 & -0.004 & 0.278 \\
        0.1 & 0.5 & 0.612 & 0.294 & -0.003 & 0.216 \\
        0.1 & 1.0 & 0.621 & 0.294 & -0.005 & 0.265 \\
        0.1 & 1.5 & 0.637 & 0.294 & 0.000 & 0.277 \\
        \textbf{0.2} & \textbf{0.5} & \textbf{0.642} & \textbf{0.319} & \textbf{0.008} & \textbf{0.275} \\
        0.2 & 1.0 & 0.632 & 0.294 & 0.002 & 0.209 \\
        0.2 & 1.5 & 0.603 & 0.275 & -0.003 & 0.234 \\
        \hline
    \end{tabular}
\end{table}

\textbf{Observations from Sample-Based Tuning:}
\begin{enumerate}
    \item All configurations achieve NMI $>$ 0.59 on the sample, far exceeding random chance
    \item The best configuration on the sample is $\lambda_{consensus}=0.2$, $\lambda_{tag}=0.5$
    \item Higher $\lambda_{tag}$ values do not consistently improve performance
\end{enumerate}

\textbf{Limitations:} These results are from only 160 images. Small sample sizes can produce:
\begin{itemize}
    \item Artificially inflated metrics (easier to cluster few samples)
    \item High variance between runs
    \item Results that do not generalize to full-scale evaluation
\end{itemize}

\section{Full-Scale Results}

\subsection{Current State}

Full-scale experiments on the complete dataset (N=37,322) were conducted using the hyperparameters tuned on the sample data. \textbf{The results show that the approach failed to scale}: while the sample achieved NMI=0.642, the full-scale experiment achieved only NMI=0.012 and ACC=0.038 with the same configuration. This represents a critical negative result that indicates hyperparameters tuned on small samples do not generalize to the full dataset.

\begin{table}[H]
    \centering
    \caption{Sample vs Full-Scale Performance Comparison}
    \begin{tabular}{lcccc}
        \hline
        \textbf{Scale} & \textbf{N} & \textbf{NMI} & \textbf{ACC} & \textbf{Status} \\
        \hline
        Sample & 160 & 0.642 & -- & Promising \\
        Full-scale & 37,322 & 0.012 & 0.038 & \textbf{Failed} \\
        \hline
    \end{tabular}
\end{table}

\subsection{Comparison: Sample vs. Full Dataset}

\begin{table}[H]
    \centering
    \caption{Comparison of sample-based tuning vs. earlier full-dataset runs}
    \label{tab:sample_vs_full}
    \begin{tabular}{lccc}
        \hline
        \textbf{Experiment} & \textbf{N} & \textbf{NMI} & \textbf{ACC} \\
        \hline
        Sample tuning (best config) & 160 & 0.642 & 0.319 \\
        Earlier full-dataset run & 37,322 & 0.012 & 0.038 \\
        \hline
    \end{tabular}
\end{table}

The large discrepancy suggests either:
\begin{enumerate}
    \item Small samples give misleadingly good metrics (pessimistic)
    \item Full-scale runs used suboptimal hyperparameters
\end{enumerate}

\section{Training Dynamics}

\subsection{Loss Curves}

Figure~\ref{fig:training_loss_ae} shows the training loss for the baseline autoencoder.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/results_ae_loss.png}
    \caption{Training loss components for the Constrained Autoencoder (CAE) over 30 epochs.
    Blue: total loss ($\mathcal{L}_{total}$), orange: reconstruction loss ($\mathcal{L}_{recon}$), green: tag prediction loss ($\mathcal{L}_{tag}$) with weight $\lambda_{tag} = 0.5$. All components show decreasing trends, converging by approximately epoch 20.}
    \label{fig:training_loss_ae}
\end{figure}

\textbf{Observation:} The AE reconstruction loss converges well, suggesting the autoencoder architecture is capable of learning meaningful representations at least for reconstruction purposes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/results_cae_loss.png}
    \caption{Training loss for Constrained Autoencoder (CAE). Multiple loss components are shown: total loss (blue), reconstruction loss, and tag prediction loss. All components show decreasing trends over 30 epochs. [to be updated]}
    \label{fig:training_loss_cae}
\end{figure}

\textbf{Observation:} The CAE loss curves show reasonable convergence, with the total loss decreasing from approximately 0.29 to 0.22. The reconstruction and tag losses both decrease, suggesting the multi-task objective is being optimized.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/results_deccs_loss.png}
    \caption{Training loss for DECCS mode. \textbf{Notable pattern:} Periodic spikes occur every 5 epochs, corresponding to when the consensus matrix is rebuilt. The spikes indicate that the new consensus targets temporarily increase the loss before the model adapts.}
    \label{fig:training_loss_deccs}
\end{figure}

\textbf{Critical Observation:} The DECCS loss curve shows a concerning pattern:
\begin{itemize}
    \item Periodic spikes every 5 epochs when consensus matrix is rebuilt
    \item The spike magnitude does not meaningfully decrease over time
    \item This suggests the consensus matrix updates may be causing training instability
    \item The loss oscillates between approximately 0.22 and 0.34
\end{itemize}

This instability in training dynamics is likely contributing to the poor clustering performance.

\section{Embedding Space Visualization}

\subsection{PCA Projections}

PCA projections reveal how each training objective shapes the embedding geometry. Figures~\ref{fig:pca_ae}--\ref{fig:pca_deccs} show the first two principal components of the learned embeddings.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/results_ae_pca.png}
    \caption{PCA projection of baseline autoencoder embeddings. The distribution forms a dense elliptical core with a prominent tail of outliers extending along PC1 (teal points, reaching PC1$\approx$40). These outliers likely represent visually distinctive samples that the reconstruction objective maps to extreme embedding values. Cluster colors are heavily mixed within the core, indicating the reconstruction-only objective does not naturally separate semantic categories.}
    \label{fig:pca_ae}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/results_cae_pca.png}
    \caption{PCA projection of Constrained Autoencoder (CAE) embeddings. Tag supervision reshapes the embedding geometry into an L-shaped distribution: a vertical arm extending upward (blue points, PC2$\approx$20) and a horizontal arm extending rightward (teal points, PC1$\approx$40). This bifurcation suggests the tag prediction branch creates distinct embedding regions for samples with different dominant attributes. However, cluster colors remain mixed in the dense core, indicating semantic supervision alone does not fully resolve cluster boundaries.}
    \label{fig:pca_cae}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/results_deccs_pca.png}
    \caption{PCA projection of DECCS embeddings. The consensus mechanism produces a curved, arc-shaped distribution spanning PC1$\in$[-12, 18] and PC2$\in$[-25, 7]. Unlike AE and CAE, some spatial organization by cluster is visible: green/yellow points concentrate on the right, blue points in the upper region, and purple outliers extend downward. The arc geometry suggests the consensus loss constrains embeddings to a lower-dimensional manifold while encouraging cluster separation.}
    \label{fig:pca_deccs}
\end{figure}

\textbf{Comparative Analysis:}
\begin{itemize}
    \item \textbf{Geometry}: Each objective produces distinct embedding shapes---elliptical (AE), L-shaped (CAE), and arc-shaped (DECCS)---demonstrating how loss functions shape representation geometry.
    \item \textbf{Outliers}: All methods produce outliers, but their distribution differs. AE and CAE show linear tails along PC1, while DECCS outliers extend along PC2, suggesting the consensus mechanism handles edge cases differently.
    \item \textbf{Cluster separation}: Only DECCS shows visible spatial grouping by cluster color. AE and CAE embeddings have heavily mixed colors, confirming that reconstruction and tag supervision alone are insufficient for cluster-aligned representations.
    \item \textbf{Variance explained}: The spread along PC1 is largest for AE/CAE ($\sim$50 units) and more compact for DECCS ($\sim$30 units), suggesting consensus learning produces more regularized embeddings.
\end{itemize}

\subsection{t-SNE Visualization}

t-SNE \citep{vanderMaaten2014} provides nonlinear dimensionality reduction that preserves local neighborhood structure, making it well-suited for visualizing cluster relationships that PCA may miss.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/results_tsne.png}
    \caption{t-SNE projection of DECCS embeddings colored by cluster assignment (color bar indicates cluster IDs 0--30). The visualization reveals clear local structure: peripheral regions contain well-separated, single-color clusters (e.g., bright green at far left, orange at lower left, cyan at upper right), while the central region shows more mixing. This pattern is consistent with NMI=0.642---the clustering captures meaningful structure but does not perfectly align with all ground truth boundaries.}
    \label{fig:tsne}
\end{figure}

\textbf{t-SNE Analysis:}
\begin{itemize}
    \item \textbf{Peripheral clusters}: Several clusters form distinct, well-separated groups at the edges of the projection (e.g., positions (-75, -10), (-50, -60), (75, -25)). These likely correspond to visually and semantically distinctive animal categories.
    \item \textbf{Central mixing}: The central region (coordinates near origin) shows interleaved colors, indicating overlapping clusters. This may reflect animals that share visual features or semantic attributes across class boundaries.
    \item \textbf{Cluster coherence}: Most clusters form spatially contiguous regions rather than scattered points, suggesting the learned embeddings capture meaningful similarity structure.
\end{itemize}

\section{Identified Challenges}

The full-scale experiments revealed several significant challenges that prevented meaningful clustering:

\subsection{Training Dynamics in DECCS Mode}

The DECCS loss curve (Figure~\ref{fig:training_loss_deccs}) shows periodic patterns corresponding to consensus matrix rebuilding every 5 epochs. While this causes temporary loss increases, the overall trend shows convergence:

\begin{itemize}
    \item Periodic fluctuations occur at epochs 5, 10, 15, etc.
    \item Between rebuilds, loss decreases consistently
    \item Final loss stabilizes, indicating the model adapts to consensus targets
\end{itemize}

\subsection{ARI Near Zero}

The Adjusted Rand Index remains close to zero despite good NMI and ACC scores. This suggests:

\begin{enumerate}
    \item Cluster boundaries do not perfectly align with class boundaries
    \item The clustering may group visually similar animals across different classes
    \item With NMI=0.012 and ACC=0.038 on the full dataset, the cluster assignments are essentially random with respect to ground truth classes
\end{enumerate}

\subsection{Sensitivity to Hyperparameters}

The grid search revealed that performance varies with hyperparameter choices:
\begin{itemize}
    \item On the sample subset (N=160), NMI ranges from 0.597 to 0.642 across configurations. However, the best configuration achieved only NMI=0.012 on the full dataset, demonstrating that hyperparameters tuned on small samples do not generalize
    \item Higher $\lambda_{tag}$ values can hurt performance
    \item This highlights the importance of systematic hyperparameter tuning
\end{itemize}

\section{Summary of Results}

\begin{table}[H]
    \centering
    \caption{Summary of implementation status and results}
    \label{tab:status_summary}
    \begin{tabular}{lcc}
        \hline
        \textbf{Component} & \textbf{Status} & \textbf{Working?} \\
        \hline
        Data loading pipeline & Implemented & \cmark \\
        Autoencoder training & Implemented & \cmark \\
        CAE with tag supervision & Implemented & \cmark \\
        Consensus matrix construction & Implemented & \cmark \\
        DECCS training loop & Implemented & \cmark \\
        Hyperparameter tuning & Completed & \cmark \\
        Clustering evaluation & Implemented & \cmark \\
        Cluster visualization & Implemented & \cmark \\
        Meaningful clustering results (NMI $>$ 0.6) & Not Achieved & X \\
        \hline
    \end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{itemize}
    \item The implementation pipeline is complete and functional
    \item The best configuration achieves NMI=0.642, ACC=0.319, Silhouette=0.275, in sample experiments
    \item Training dynamics show expected behavior with periodic consensus updates
\end{itemize}

The following chapter discusses these results in detail and their implications.