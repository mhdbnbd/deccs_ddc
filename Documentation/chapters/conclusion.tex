\section{Summary of Work}

This thesis investigated the integration of Deep Descriptive Clustering (DDC) principles into the Deep Embedded Clustering with Consensus Representations (DECCS) framework to address the challenge of interpretability in deep clustering systems.

\subsection{What Was Accomplished}

\begin{enumerate}
    \item \textbf{Framework Design}: We proposed an architecture that combines DECCS's consensus-based clustering with DDC's semantic supervision through a multi-objective loss function.

    \item \textbf{Implementation}: We developed a complete experimental pipeline including modular model architectures, ensemble clustering with consensus matrix construction, and visualization tools.

    \item \textbf{Hyperparameter Analysis}: We conducted grid search experiments on sample data, achieving NMI=0.642 on a subset of 160 samples.
\end{enumerate}

\subsection{What Did Not Work}

\begin{enumerate}
    \item \textbf{Full-Scale Failure}: When applied to the full AwA2 dataset (N=37,322), the approach achieved only NMI=0.012 and ACC=0.038---effectively random clustering.

    \item \textbf{Hyperparameter Transfer}: Configurations tuned on small samples did not generalize to the full dataset.

    \item \textbf{Incomplete Integration}: We did not implement DDC's pairwise consistency loss, which may be essential for the approach to work.
\end{enumerate}

\begin{table}[H]
    \centering
    \caption{Summary of Results}
    \begin{tabular}{lccc}
        \hline
        \textbf{Scale} & \textbf{NMI} & \textbf{ACC} & \textbf{Status} \\
        \hline
        Sample (N=160) & 0.642 & -- & Promising \\
        Full-scale (N=37,322) & 0.012 & 0.038 & \textbf{Failed} \\
        \hline
    \end{tabular}
\end{table}

\section{Research Questions Answered}

Returning to our original research objectives:

\subsection{RQ1: How can DDC be integrated into DECCS?}

We proposed an integration through:
\begin{itemize}
    \item \textbf{Architectural modification}: Adding a tag prediction branch to the autoencoder encoder
    \item \textbf{Loss augmentation}: Incorporating tag prediction loss (BCE) alongside reconstruction loss
    \item \textbf{Consensus integration}: Building sparse consensus matrices from ensemble clusterings
\end{itemize}

\textbf{However, the integration is incomplete}---we did not implement DDC's pairwise consistency loss, and the full-scale results suggest fundamental issues with the current approach.

\subsection{RQ2: Impact on Clustering Performance}

\textbf{Cannot be conclusively answered.} With NMI=0.012 on the full dataset, we cannot draw meaningful conclusions about whether semantic supervision improves or harms clustering performance. The approach failed before this question could be properly evaluated.

\subsection{RQ3: Performance on AwA2 Dataset}

On the Animals with Attributes 2 dataset:
\begin{itemize}
    \item \textbf{Sample data (N=160)}: NMI=0.642, showing promise
    \item \textbf{Full-scale (N=37,322)}: NMI=0.012, ACC=0.038---\textbf{effectively random}
\end{itemize}

The approach failed to achieve meaningful clustering on the full dataset.

\section{Lessons Learned}

This work provides important lessons for deep clustering research:

\subsection{Full-Scale Validation is Essential}

Promising results on small samples do not guarantee scalability. Our sample-scale results (NMI=0.642) would have suggested success, but full-scale evaluation revealed fundamental failure (NMI=0.012).

\subsection{Multi-Objective Optimization is Difficult}

Combining reconstruction, semantic supervision, and consensus clustering creates a complex optimization landscape. The interaction between these objectives at scale is not well understood.

\subsection{Negative Results Have Value}

This thesis documents an approach that did not work. While less satisfying than reporting success, honest negative results help the research community avoid repeating unsuccessful approaches and understand the difficulty of the problem.

\section{Limitations}

\begin{enumerate}
    \item \textbf{Scaling Failure}: The approach failed to produce meaningful clustering on the full dataset.

    \item \textbf{Incomplete Integration}: Missing DDC's pairwise consistency loss, which may be essential.

    \item \textbf{Single Dataset}: Only evaluated on AwA2.

    \item \textbf{Potential Bugs}: The discrepancy between t-SNE visualizations (showing some structure) and clustering metrics (near-random) suggests possible metric calculation issues.

    \item \textbf{Fixed Architecture}: Simple convolutional autoencoder may be insufficient for complex images.
\end{enumerate}

\section{Future Work}

Based on our negative results, the following work is needed before the approach can be considered viable:

\subsection{Immediate Priorities}

\begin{enumerate}
    \item \textbf{Implement Pairwise Consistency Loss}: DDC's pairwise loss may be essential for aligning clustering with semantic supervision.

    \item \textbf{Investigate Metric Calculation}: The discrepancy between t-SNE visualizations and clustering metrics warrants investigation of potential bugs.

    \item \textbf{Use Pre-trained Backbone}: A pre-trained feature extractor (ResNet, ViT) may provide better representations than training from scratch.

    \item \textbf{EMA Consensus Updates}: Exponential moving average for consensus matrix updates may improve stability.
\end{enumerate}

\subsection{Longer-Term Research}

\begin{enumerate}
    \item \textbf{Progressive Scaling}: Develop methods to gradually increase dataset size during training.

    \item \textbf{Alternative Loss Balancing}: Explore gradient-based methods for balancing multi-objective optimization.

    \item \textbf{Theoretical Analysis}: Understand why sample-scale success does not transfer to full-scale.
\end{enumerate}

\section{Final Remarks}

This thesis set out to integrate Deep Descriptive Clustering principles into the DECCS framework for interpretable deep clustering. While we successfully implemented the framework and achieved promising results on sample data (NMI=0.642 on N=160), \textbf{the approach fundamentally failed when applied to the full dataset} (NMI=0.012 on N=37,322).

This negative result is honest and important. It demonstrates that:
\begin{itemize}
    \item Integrating complex deep clustering frameworks is more difficult than it appears
    \item Sample-scale validation is insufficient---full-scale experiments are essential
    \item Multi-objective optimization in deep clustering remains a challenging open problem
\end{itemize}

The vision of clustering that is both accurate and interpretable remains compelling. However, achieving this vision requires more careful methodology than our current approach provides. We hope this honest reporting of failure contributes to the research community's understanding of the challenges involved in interpretable deep clustering.